{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Technical Setup\n",
    "\n"
   ],
   "id": "8d911347b26d6c5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import of Demo Translations\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe.\n"
   ],
   "id": "419979966a0573e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "from mlflow import transformers\n",
    "\n",
    "with open('machine_translation.pkl', 'rb') as f:\n",
    "    # Einlesen des Dataframes\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data)"
   ],
   "id": "8a50c8d46ced9e04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import of AI-Models\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp: \n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python\n",
    "\n"
   ],
   "id": "cd6f25627718f266"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "gemma = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-ai/gemma-2b-it-GGUF\",\n",
    "\tfilename=\"gemma-2b-it-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "id": "34aceb64230259ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llama32 = Llama.from_pretrained(\n",
    "\trepo_id=\"hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\",\n",
    "\tfilename=\"llama-3.2-3b-instruct-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,)\n"
   ],
   "id": "ff913d69898d5e12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llama31 = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "\tfilename=\"Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\",\n",
    "    verbose=False,)\n"
   ],
   "id": "8727cb5bdc6fc03e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "aya23 = Llama.from_pretrained(\n",
    "\trepo_id=\"bartowski/aya-23-35B-GGUF\",\n",
    "\tfilename=\"aya-23-35B-Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "id": "3ab9518f20e74abd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output = llama31(\n",
    "\t\"Q: Übersetze mir folgenden Text ins Englische: 'Hallo ich heiße Peter.'\",\n",
    "\tmax_tokens=512,\n",
    "\techo=False,\n",
    "    stop=[\"Q:\"]\n",
    ")\n",
    "\n",
    "# print(output)\n",
    "print(output['choices'][0]['text'])"
   ],
   "id": "444314dbf62129ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output2 = aya23(\n",
    "\t\"Q: Übersetze mir folgenden Text ins Englische: 'Hallo ich heiße Peter.'\",\n",
    "\tmax_tokens=512,\n",
    "\techo=False,\n",
    "    stop=[\"Q:\"]\n",
    ")\n",
    "\n",
    "# print(output)\n",
    "print(output2['choices'][0]['text'])"
   ],
   "id": "7d979a63b1942a90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(output)",
   "id": "ee8b6ec6615e328f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"check-localhost-connection\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"foo\", 1)\n",
    "    mlflow.log_metric(\"bar\", 3)"
   ],
   "id": "a76c826c6992583a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MetricX Score berechnen\n",
    "https://github.com/google-research/metricx"
   ],
   "id": "d9b372eab89c28de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:24:42.876278Z",
     "start_time": "2025-01-09T16:24:42.866844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    \"\"\"\n",
    "\n",
    "    data = [{\"id\": \"1\", \"source\": source, \"reference\": reference, \"hypothesis\": hypothesis}]\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = \"./temp_input.jsonl\"\n",
    "    output_file = \"./temp_output.jsonl\"\n",
    "    model = \"google/metricx-24-hybrid-large-v2p6-bfloat16\"\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        command = [\n",
    "            \"python\", \"-m\", \"metricx24.predict\",\n",
    "            \"--tokenizer\", \"google/mt5-xl\",\n",
    "            \"--model_name_or_path\", model,\n",
    "            \"--max_input_length\", \"1536\",\n",
    "            \"--batch_size\", \"1\",\n",
    "            \"--input_file\", input_file,\n",
    "            \"--output_file\", output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        # for line in process.stdout:\n",
    "        #     print(line, end=\"\")\n",
    "        # for line in process.stderr:\n",
    "        #     print(f\"ERROR: {line}\", end=\"\")\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "             print(f\"Error executing metricx24. Return code: {process.returncode}\")\n",
    "             return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get(\"prediction\"))  # Ensure that \"score\" exists\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print(\"Error parsing the output file.\")\n",
    "                    return None\n",
    "\n",
    "        return None # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass #If the files don't exist for some reason, the error is caught\n"
   ],
   "id": "27a0b1a818fdc3b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:25:05.443858Z",
     "start_time": "2025-01-09T16:24:46.718860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example call\n",
    "source_text = \"I am learning Python for Machine Learning.\"\n",
    "reference_text = \"I am learning Python for machine learning.\"\n",
    "hypothesis_text = \"I'm studying Python for machine learning.\"\n",
    "\n",
    "score = calculate_metricx_score(source_text, reference_text, hypothesis_text)\n",
    "print(score)"
   ],
   "id": "e9650490a5f31d60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1640625\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## zweite Möglichkeit metricx zu nutzen...\n",
    "import sys\n",
    "from metricx24 import predict\n",
    "\n",
    "# Simuliere Kommandozeilenargumente\n",
    "sys.argv = [\n",
    "    \"predict.py\",  # Name des Skripts (wird ignoriert)\n",
    "    \"--tokenizer\", \"google/mt5-xl\",\n",
    "    \"--model_name_or_path\", \"google/metricx-24-hybrid-large-v2p6-bfloat16\",\n",
    "    \"--max_input_length\", \"1536\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--input_file\", \"./input.jsonl\",\n",
    "    \"--output_file\", \"./output.jsonl\"\n",
    "]\n",
    "\n",
    "# Rufe die main-Funktion auf\n",
    "predict.main()\n",
    "\n",
    "print(\"Vorhersage abgeschlossen. Ergebnisse sind in 'output.jsonl' gespeichert.\")"
   ],
   "id": "8d261ae145bb278c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
