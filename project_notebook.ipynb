{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Technical Setup\n",
    "\n"
   ],
   "id": "25397a2bcb1823f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import of Demo Translations\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe.\n"
   ],
   "id": "521846f0167f6127"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T12:54:19.837993Z",
     "start_time": "2024-12-23T12:54:19.799500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('machine_translation.pkl', 'rb') as f:\n",
    "    # Einlesen des Dataframes\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    complexity                                        text_german  \\\n",
      "0         easy  Felix hat es satt: Ständig ist Mama unterwegs....   \n",
      "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
      "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
      "3  pop_science  Dass der Klimawandel die Hitzewellen in Südasi...   \n",
      "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
      "\n",
      "                                        text_english  \n",
      "0  Felix is fed up: Mom is always on the go. But ...  \n",
      "1  The approximately 1,400 deployed officers have...  \n",
      "2  The head of state also has the right to appoin...  \n",
      "3  There is no question that climate change is in...  \n",
      "4  The DSA-110, situated at the Owens Valley Radi...  \n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import of AI-Models\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp: \n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python\n",
    "\n"
   ],
   "id": "ed18a088f866a163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:54:20.350080Z",
     "start_time": "2024-12-23T12:54:19.854110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "gemma = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-ai/gemma-2b-it-GGUF\",\n",
    "\tfilename=\"gemma-2b-it-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "id": "465e18af3ad7a965",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:54:20.895881Z",
     "start_time": "2024-12-23T12:54:20.356884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama32 = Llama.from_pretrained(\n",
    "\trepo_id=\"hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\",\n",
    "\tfilename=\"llama-3.2-3b-instruct-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,)\n"
   ],
   "id": "fce7e4c6bc770f24",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:54:54.401607Z",
     "start_time": "2024-12-23T12:54:53.838377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama31 = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "\tfilename=\"Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\",\n",
    "    verbose=False,)\n"
   ],
   "id": "bf2e6793e18bb549",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:54:22.582444Z",
     "start_time": "2024-12-23T12:54:21.705598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aya23 = Llama.from_pretrained(\n",
    "\trepo_id=\"bartowski/aya-23-35B-GGUF\",\n",
    "\tfilename=\"aya-23-35B-Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "id": "84de53cf451920f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:58:07.974572Z",
     "start_time": "2024-12-23T12:54:58.115864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = llama31(\n",
    "\t\"Q: Übersetze mir folgenden Text ins Englische: 'Hallo ich heiße Peter.'\",\n",
    "\tmax_tokens=512,\n",
    "\techo=False,\n",
    "    stop=[\"Q:\"]\n",
    ")\n",
    "\n",
    "# print(output)\n",
    "print(output['choices'][0]['text'])"
   ],
   "id": "f405f223c94046c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Der Text ist ein Satz.\n",
      "A: Der Übersetzungsversuch wäre: 'Hello, my name is Peter.'\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T13:09:52.131477Z",
     "start_time": "2024-12-23T12:58:45.394762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output2 = aya23(\n",
    "\t\"Q: Übersetze mir folgenden Text ins Englische: 'Hallo ich heiße Peter.'\",\n",
    "\tmax_tokens=512,\n",
    "\techo=False,\n",
    "    stop=[\"Q:\"]\n",
    ")\n",
    "\n",
    "# print(output)\n",
    "print(output2['choices'][0]['text'])"
   ],
   "id": "19c424389bdb94a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 'Hello, my name is Peter.'\n",
      "A: Hello, my name is Peter.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:54:30.396745Z",
     "start_time": "2024-12-23T12:20:26.113122Z"
    }
   },
   "cell_type": "code",
   "source": "print(output)",
   "id": "3f2aed159bbfe5d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-e8e08548-44ce-4894-8e4f-024d46a01dfe', 'object': 'text_completion', 'created': 1734956186, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/./Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf', 'choices': [{'text': \" Der Text ist ein Satz.\\nA: Der Übersetzungsversuch wäre: 'Hello, my name is Peter.'\\n\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 22, 'completion_tokens': 28, 'total_tokens': 50}}\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
