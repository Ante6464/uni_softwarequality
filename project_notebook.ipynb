{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Technical Setup\n",
    "\n"
   ],
   "id": "8d911347b26d6c5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import of Demo Translations\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe.\n"
   ],
   "id": "419979966a0573e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "from mlflow import transformers\n",
    "\n",
    "with open('machine_translation.pkl', 'rb') as f:\n",
    "    # Einlesen des Dataframes\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data)"
   ],
   "id": "8a50c8d46ced9e04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import of AI-Models\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp: \n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python\n",
    "\n"
   ],
   "id": "cd6f25627718f266"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "gemma = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-ai/gemma-2b-it-GGUF\",\n",
    "\tfilename=\"gemma-2b-it-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "id": "34aceb64230259ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llama32 = Llama.from_pretrained(\n",
    "\trepo_id=\"hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\",\n",
    "\tfilename=\"llama-3.2-3b-instruct-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,)\n"
   ],
   "id": "ff913d69898d5e12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llama31 = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "\tfilename=\"Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\",\n",
    "    verbose=False,)\n"
   ],
   "id": "8727cb5bdc6fc03e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "aya23 = Llama.from_pretrained(\n",
    "\trepo_id=\"bartowski/aya-23-35B-GGUF\",\n",
    "\tfilename=\"aya-23-35B-Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "id": "3ab9518f20e74abd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output = llama31(\n",
    "\t\"Q: Übersetze mir folgenden Text ins Englische: 'Hallo ich heiße Peter.'\",\n",
    "\tmax_tokens=512,\n",
    "\techo=False,\n",
    "    stop=[\"Q:\"]\n",
    ")\n",
    "\n",
    "# print(output)\n",
    "print(output['choices'][0]['text'])"
   ],
   "id": "444314dbf62129ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output2 = aya23(\n",
    "\t\"Q: Übersetze mir folgenden Text ins Englische: 'Hallo ich heiße Peter.'\",\n",
    "\tmax_tokens=512,\n",
    "\techo=False,\n",
    "    stop=[\"Q:\"]\n",
    ")\n",
    "\n",
    "# print(output)\n",
    "print(output2['choices'][0]['text'])"
   ],
   "id": "7d979a63b1942a90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(output)",
   "id": "ee8b6ec6615e328f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"check-localhost-connection\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"foo\", 1)\n",
    "    mlflow.log_metric(\"bar\", 3)"
   ],
   "id": "a76c826c6992583a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MetricX Score berechnen\n",
    "https://github.com/google-research/metricx"
   ],
   "id": "d9b372eab89c28de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T11:19:00.514556Z",
     "start_time": "2025-01-09T11:18:35.829540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from metricx24 import predict\n",
    "\n",
    "# Simuliere Kommandozeilenargumente\n",
    "sys.argv = [\n",
    "    \"predict.py\",  # Name des Skripts (wird ignoriert)\n",
    "    \"--tokenizer\", \"google/mt5-xl\",\n",
    "    \"--model_name_or_path\", \"google/metricx-24-hybrid-large-v2p6-bfloat16\",\n",
    "    \"--max_input_length\", \"1536\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--input_file\", \"./input.jsonl\",\n",
    "    \"--output_file\", \"./output.jsonl\"\n",
    "]\n",
    "\n",
    "# Rufe die main-Funktion auf\n",
    "predict.main()\n",
    "\n",
    "print(\"Vorhersage abgeschlossen. Ergebnisse sind in 'output.jsonl' gespeichert.\")"
   ],
   "id": "8d261ae145bb278c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorhersage abgeschlossen. Ergebnisse sind in 'output.jsonl' gespeichert.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T11:21:36.149982Z",
     "start_time": "2025-01-09T11:21:00.737530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "data = [\n",
    "    {\"id\": \"1\", \"source\": \"Ich lerne Python für Machine Learning.\", \"reference\": \"I am learning Python for machine learning.\", \"hypothesis\": \"I'm going to the supermarket today.\"}\n",
    "]\n",
    "\n",
    "# JSONL-Datei schreiben mit doppelten Anführungszeichen\n",
    "with open(\"input.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data:\n",
    "        json.dump(entry, f)  # Wandelt das Dictionary in einen JSON-kompatiblen String um\n",
    "        f.write(\"\\n\")  # Zeilenumbruch nach jedem JSON-Objekt\n",
    "\n",
    "print(\"Die Datei 'input.jsonl' wurde erfolgreich erstellt!\")\n",
    "\n",
    "\n",
    "# Definiere den Befehl als Liste von Argumenten\n",
    "command = [\n",
    "    \"python\", \"-m\", \"metricx24.predict\",\n",
    "    \"--tokenizer\", \"google/mt5-xl\",\n",
    "    \"--model_name_or_path\", \"google/metricx-24-hybrid-large-v2p6-bfloat16\",\n",
    "    \"--max_input_length\", \"1536\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--input_file\", \"./input.jsonl\",\n",
    "    \"--output_file\", \"./output.jsonl\"\n",
    "]\n",
    "\n",
    "# Subprocess ausführen und die Ausgabe live anzeigen\n",
    "process = subprocess.Popen(\n",
    "    command,\n",
    "    stdout=subprocess.PIPE,  # Standardausgabe abfangen\n",
    "    stderr=subprocess.PIPE,  # Fehlerausgabe abfangen\n",
    "    text=True,               # Ausgabe als Text (kein Byte-Objekt)\n",
    "    bufsize=1,               # Zeilenweise Puffern\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Zeilenweise Ausgabe lesen und live anzeigen\n",
    "print(\"Live-Ausgabe des Befehls:\\n\")\n",
    "for line in process.stdout:\n",
    "    print(line, end=\"\")  # Zeilen ohne zusätzlichen Zeilenumbruch ausgeben\n",
    "\n",
    "# Fehler ausgeben (falls vorhanden)\n",
    "for line in process.stderr:\n",
    "    print(f\"ERROR: {line}\", end=\"\")\n",
    "\n",
    "# Auf den Abschluss des Prozesses warten\n",
    "process.wait()\n",
    "print(\"\\nBefehl abgeschlossen.\")"
   ],
   "id": "eeccf936918cb77f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Datei 'input.jsonl' wurde erfolgreich erstellt!\n",
      "Live-Ausgabe des Befehls:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(10256) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: /opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 38.19 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|██████████| 1/1 [00:00<00:00, 74.61 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|██████████| 1/1 [00:00<00:00, 16.75 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|██████████| 1/1 [00:00<00:00, 566.64 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|██████████| 1/1 [00:00<00:00, 2505.56it/s]\n",
      "\n",
      "Befehl abgeschlossen.\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
