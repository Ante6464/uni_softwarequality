{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jupyter Notebook for Project \"Comparison of LLM Prompting Techniques\"",
   "id": "872549f55a437d8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.094264Z",
     "start_time": "2025-01-31T23:36:24.088819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import sacrebleu\n",
    "from llama_cpp import Llama\n",
    "import time\n"
   ],
   "id": "d7cb373e19f3872c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 Data Loading\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe."
   ],
   "id": "84eebb77e3f61e83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.111850Z",
     "start_time": "2025-01-31T23:36:24.101267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_pickle('machine_translation.pkl')\n",
    "data"
   ],
   "id": "31214ca44be7863b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity                                        text_german  \\\n",
       "0         easy  Felix hat es satt: Ständig ist Mama unterwegs....   \n",
       "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
       "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
       "3  pop_science  Dass der Klimawandel die Hitzewellen in Südasi...   \n",
       "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
       "\n",
       "                                        text_english  \n",
       "0  Felix is fed up: Mom is always on the go. But ...  \n",
       "1  The approximately 1,400 deployed officers have...  \n",
       "2  The head of state also has the right to appoin...  \n",
       "3  There is no question that climate change is in...  \n",
       "4  The DSA-110, situated at the Owens Valley Radi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german</th>\n",
       "      <th>text_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>Felix hat es satt: Ständig ist Mama unterwegs....</td>\n",
       "      <td>Felix is fed up: Mom is always on the go. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>Die rund 1.400 eingesetzten Beamten haben demn...</td>\n",
       "      <td>The approximately 1,400 deployed officers have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>Der Staatschef hat zugleich aber das Recht, vo...</td>\n",
       "      <td>The head of state also has the right to appoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>Dass der Klimawandel die Hitzewellen in Südasi...</td>\n",
       "      <td>There is no question that climate change is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Der DSA-110, der sich am Owens Valley Radio Ob...</td>\n",
       "      <td>The DSA-110, situated at the Owens Valley Radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.174467Z",
     "start_time": "2025-01-31T23:36:24.162640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['complexity'] = data['complexity']\n",
    "data_info['text_german_length'] = data['text_german'].str.len()\n",
    "data_info['text_english_length'] = data['text_english'].str.len()\n",
    "data_info"
   ],
   "id": "f378daff85fa7954",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity  text_german_length  text_english_length\n",
       "0         easy                 485                  415\n",
       "1     news_gen                 296                  280\n",
       "2    news_spec                 518                  484\n",
       "3  pop_science                 542                  521\n",
       "4      science                1003                  827"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german_length</th>\n",
       "      <th>text_english_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>485</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>296</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>518</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>542</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>1003</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.263157Z",
     "start_time": "2025-01-31T23:36:24.259292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Language(Enum):\n",
    "    ENGLISH = 'English'\n",
    "    GERMAN = 'German'"
   ],
   "id": "323c0bdb3ca88317",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 2 Model Loading\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp:\n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python"
   ],
   "id": "da93308612340266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.328667Z",
     "start_time": "2025-01-31T23:36:24.325194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration of the models\n",
    "MODELS = {\n",
    "    'gemma': {\n",
    "        'repo_id': 'lmstudio-ai/gemma-2b-it-GGUF',\n",
    "        'filename': 'gemma-2b-it-q8_0.gguf',\n",
    "    },\n",
    "    'llama32': {\n",
    "        'repo_id': 'hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF',\n",
    "        'filename': 'llama-3.2-3b-instruct-q8_0.gguf',\n",
    "    },\n",
    "    'llama31': {\n",
    "        'repo_id': 'lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "        'filename': 'Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
    "    },\n",
    "    # 'aya23': {\n",
    "    #     'repo_id': 'bartowski/aya-23-35B-GGUF',\n",
    "    #     'filename': 'aya-23-35B-Q5_K_M.gguf',\n",
    "    # },\n",
    "}"
   ],
   "id": "d41d93f0a2545265",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.379799Z",
     "start_time": "2025-01-31T23:36:24.375681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_llama_model(repo_id, filename, n_ctx=None):\n",
    "    try:\n",
    "        if n_ctx is None:\n",
    "            # default of llama_cpp\n",
    "            n_ctx = 512\n",
    "        if repo_id is not None and filename is not None:\n",
    "            model = Llama.from_pretrained(\n",
    "                repo_id=repo_id,\n",
    "                filename=filename,\n",
    "                n_ctx=n_ctx,\n",
    "                # n_gpu_layers=n_gpu_layers,\n",
    "                n_threads=5,\n",
    "                verbose=False,\n",
    "            )\n",
    "            print(f\"Model {repo_id} erfolgreich geladen mit n_ctx={n_ctx}\")\n",
    "            return model\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden von {filename}: {e}\")\n",
    "        return None"
   ],
   "id": "3c03d17b6bc17b83",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "464c36e55f18159"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3 Pipeline\n",
    "\n",
    "### 3.1 Model Interaction"
   ],
   "id": "8f91fc96f9d5591e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.412209Z",
     "start_time": "2025-01-31T23:36:24.408300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(model, prompt, reference_translation):\n",
    "    # we estimate the needed max_tokens based on the tokenized prompt and reference_translation\n",
    "    token_length_ref = len(model.tokenize(reference_translation.encode('utf-8')))\n",
    "    token_length_prompt = len(model.tokenize(prompt.encode('utf-8')))\n",
    "    # the model should not need more tokens than this\n",
    "    estimated_max_tokens = (token_length_prompt + token_length_ref) * 1.5\n",
    "\n",
    "    response = model(prompt, max_tokens=estimated_max_tokens, echo=False)\n",
    "    # print(response['choices'][0]['text'])\n",
    "    return response['choices'][0]['text']"
   ],
   "id": "6420297102cf617f",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Metrics Calculation\n",
    "[GitHub Repo to MetricX](https://github.com/google-research/metricx)"
   ],
   "id": "524f90bdcc56ab38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.447279Z",
     "start_time": "2025-01-31T23:36:24.438050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    '''\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    '''\n",
    "\n",
    "    data = [{'id': '1', 'source': source, 'reference': reference, 'hypothesis': hypothesis}]\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = './temp_input.jsonl'\n",
    "    output_file = './temp_output.jsonl'\n",
    "    model = 'google/metricx-24-hybrid-large-v2p6-bfloat16'\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'w', encoding='utf-8') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        command = [\n",
    "            'python', '-m', 'metricx24.predict',\n",
    "            '--tokenizer', 'google/mt5-xl',\n",
    "            '--model_name_or_path', model,\n",
    "            '--max_input_length', '1536',\n",
    "            '--batch_size', '1',\n",
    "            '--input_file', input_file,\n",
    "            '--output_file', output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        #for line in process.stdout:\n",
    "        #    print(line, end='')\n",
    "        #for line in process.stderr:\n",
    "        #    print(f'ERROR: {line}', end='')\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f'Error executing metricx24. Return code: {process.returncode}')\n",
    "            return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get('prediction'))  # Ensure that 'score' exists\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print('Error parsing the output file.')\n",
    "                    return None\n",
    "\n",
    "        return None  # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass  #If the files don't exist for some reason, the error is caught\n",
    "\n",
    "\n"
   ],
   "id": "969f3b0640a2fdf0",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.473575Z",
     "start_time": "2025-01-31T23:36:24.468285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_translation(source, reference, hypothesis):\n",
    "    # Note that BLEU and chrF Scores can only be between 0 and 100\n",
    "    #    but sacreblue returns floats as percentage values\n",
    "    # --> so the scores are between 0 and 100)\n",
    "    bleu_score = sacrebleu.corpus_bleu([hypothesis], [[reference]]).score\n",
    "    chrf_score = sacrebleu.corpus_chrf([hypothesis], [[reference]]).score\n",
    "\n",
    "    metricx_score = calculate_metricx_score(source, reference, hypothesis)\n",
    "    if metricx_score is None:\n",
    "        metricx_score = -1\n",
    "\n",
    "    rougel_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rougel_score = rougel_scorer.score(reference, hypothesis)\n",
    "\n",
    "    return {'BLEU': bleu_score,\n",
    "            'chrF': chrf_score,\n",
    "            'rougeL': (rougel_score['rougeL'].fmeasure * 100),\n",
    "            'MetricX': metricx_score}\n"
   ],
   "id": "b2db81cb89d1a90e",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.491851Z",
     "start_time": "2025-01-31T23:36:24.487636Z"
    }
   },
   "cell_type": "code",
   "source": "#evaluate_translation(\"Felix hat es satt: Ständig ist Mama unterwegs. Doch warum das so ist, will ihm niemand verraten. Für Felix ist daher klar: Seine Mutter ist eine Geheimagentin. Als er an seinem zehnten Geburtstag einen rätselhaften Brief erhält, scheint sich seine Vermutung zu bestätigen. Zusammen mit seiner besten Freundin Lina macht er sich daran, das Geheimnis um Mamas Arbeit zu lüften. Ehe sie sich versehen, stecken die beiden mitten in ihrem ersten spannenden Fall als angehende Geheimagenten.\", \"Felix is fed up: Mom is always on the go. But nobody will tell him why that is. For Felix, it's clear: his mother is a secret agent. When he receives a mysterious letter on his tenth birthday, his suspicion seems to be confirmed. Together with his best friend Lina, he sets out to uncover the secret of mom's job. Before they know it, the two are in the middle of their first exciting case as budding secret agents.\", \"\\n\\n**English Translation:**\\n\\nFelix had sat: Mama was constantly on the go. But why this is the case, no one will tell him. Therefore, clear to Felix: his mother is a covert agent. When he receives a cryptic letter on his eleventh birthday, it seems his suspicion is confirmed. Together with his best friend Lina, he starts unraveling the mystery of his mother's job. When they finally manage to solve the case, they stick to their first exciting clue like detectives.\")",
   "id": "3cf725633e2ed7e8",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Logging to MLFLow",
   "id": "850ab5dc61d6bfd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.511515Z",
     "start_time": "2025-01-31T23:36:24.505510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, target_language, tmp_result,\n",
    "                  prompt_language):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    if experiment:\n",
    "        if experiment.lifecycle_stage == 'deleted':\n",
    "            mlflow.tracking.MlflowClient().restore_experiment(experiment.experiment_id)\n",
    "            #mlflow.delete_experiment(experiment.experiment_id)\n",
    "    else:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f'{model_name}/{complexity}/{prompt_type}'):\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('complexity', complexity)\n",
    "        mlflow.log_param('prompt_type', prompt_type)\n",
    "        mlflow.log_param('target_language', target_language)\n",
    "        mlflow.log_param('prompt_language', prompt_language)\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        tmp_result.to_json('tmp_results.json', index=False)\n",
    "        mlflow.log_artifact('tmp_results.json')\n",
    "        mlflow.end_run()\n"
   ],
   "id": "ea6b2fc077799b49",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Pipeline Composition",
   "id": "525e76790b8b3239"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.536780Z",
     "start_time": "2025-01-31T23:36:24.523255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "def run_pipeline(texts):\n",
    "    results = pd.DataFrame(\n",
    "        columns=['model', 'complexity', 'prompt_type', 'prompt', 'source_text', 'hypothesis', 'reference', 'metrics',\n",
    "                 'prompt_language'])\n",
    "    mlflow.set_tracking_uri(uri='http://127.0.0.1:5000')\n",
    "\n",
    "    for model_name, model_config in MODELS.items():\n",
    "        for _, row in texts.iterrows():\n",
    "            model = createModel(model_config, row)\n",
    "\n",
    "            # Übersetzung Deutsch -> Englisch\n",
    "            for prompt_type, template_data in PROMPT_TEMPLATES_GERMAN_ENGLISH.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_german']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, Language.ENGLISH, results,\n",
    "                                                 row['text_german'], row['text_english'], template_data)\n",
    "\n",
    "            # Übersetzung Englisch -> Deutsch\n",
    "            for prompt_type, template_data in PROMPT_TEMPLATES_ENGLISH_GERMAN.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_english']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, Language.GERMAN, results,\n",
    "                                                 row['text_english'], row['text_german'], template_data)\n",
    "\n",
    "            # we dont need the model anymore so we delete it\n",
    "            del model\n",
    "            gc.collect()\n",
    "    return results\n",
    "\n",
    "\n",
    "def createModel(model_config, row):\n",
    "    # at first we just use the dummyModel for tokenization\n",
    "    dummyModel = create_llama_model(model_config['repo_id'], model_config['filename'])\n",
    "    combined_text = f\"{row['text_german']} {row['text_english']}\"\n",
    "    text_tokens = len(dummyModel.tokenize(combined_text.encode('utf-8')))\n",
    "    # we want to tokenize the longest template\n",
    "    max_promp_template = max(\n",
    "        (t['template'] for d in (PROMPT_TEMPLATES_GERMAN_ENGLISH, PROMPT_TEMPLATES_ENGLISH_GERMAN) for t in d.values()),\n",
    "        key=len)\n",
    "    prompt_tokens = len(dummyModel.tokenize(max_promp_template.encode('utf-8')))\n",
    "\n",
    "    # now we delete the dummyModel and then create the final model based on the estimated_max_tokens\n",
    "    del dummyModel\n",
    "    gc.collect()\n",
    "    estimated_max_tokens = (text_tokens + prompt_tokens) * 2\n",
    "    n_ctx = int(estimated_max_tokens * 1.2)\n",
    "    print(f\"estimated_max_tokens: {estimated_max_tokens}; n_ctx: {n_ctx}\")\n",
    "    model = create_llama_model(model_config['repo_id'], model_config['filename'], n_ctx=n_ctx)\n",
    "    return model\n",
    "\n",
    "\n",
    "def execute_mlflow_run(complexity, model, model_name, prompt_type, target_language: Language, results, source_text,\n",
    "                       reference_text, template_data):\n",
    "    prompt = template_data['template'].format(text=source_text)\n",
    "    prompt_language = template_data['prompt_language']\n",
    "\n",
    "    start_time_translation = time.time()\n",
    "    hypothesis = translate(model, prompt, reference_text)\n",
    "    end_time_translation = time.time()\n",
    "    print('Prompt finished in (seconds): ', round(end_time_translation - start_time_translation, 2))\n",
    "    metrics = evaluate_translation(source=source_text, reference=reference_text, hypothesis=hypothesis)\n",
    "    print('Metric Calculation in (seconds): ', round(time.time() - end_time_translation, 2))\n",
    "\n",
    "    tmp_result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'complexity': complexity,\n",
    "        'prompt_type': prompt_type,\n",
    "        'prompt': prompt,\n",
    "        'source_text': source_text,\n",
    "        'hypothesis': hypothesis,\n",
    "        'reference_text': reference_text,\n",
    "        'metrics': metrics,\n",
    "        'prompt_language': prompt_language.value  # Hier .value für den Stringwert\n",
    "    }])\n",
    "\n",
    "    # MLflow-Logging\n",
    "    experiment_name = f'{model_name}_{complexity}'\n",
    "\n",
    "    log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, target_language.value, tmp_result,\n",
    "                  prompt_language.value)\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    results = pd.concat([\n",
    "        results,\n",
    "        tmp_result\n",
    "    ], ignore_index=True)\n",
    "    return results"
   ],
   "id": "961cb868381bd8a6",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 Prompt Composition\n",
   "id": "c1d3b0fe8482b687"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T23:36:24.554541Z",
     "start_time": "2025-01-31T23:36:24.549077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Verschiedene Prompt Arten: zero-shot, few-shot und verschiedene Variationen reinbringen\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES_ENGLISH_GERMAN = {\n",
    "    #'zero_shot_to-german_english_1': {\n",
    "    #    'template': 'Please translate the following text from English to German: \\\"{text}\\\"',\n",
    "    #    'prompt_language': Language.ENGLISH\n",
    "    #},\n",
    "    #'zero_shot_to-german_german_1': {\n",
    "    #    'template': 'Bitte übersetze diesen Text von Englisch nach Deutsch: \\\"{text}\\\"',\n",
    "    #    'prompt_language': Language.GERMAN\n",
    "    #},\n",
    "    'zero_shot_to-german_english_2': {\n",
    "        'template': 'Please translate the following text from English to German. Do not ask questions, just return the translation. This is the text: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.ENGLISH\n",
    "    },\n",
    "    'zero_shot_to-german_german_2': {\n",
    "        'template': 'Bitte übersetze diesen Text von Englisch nach Deutsch. Stelle keine Gegenfragen, sondern gib einfach die Übersetzung aus. Das ist der Text: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.GERMAN\n",
    "    },\n",
    "    # ... weitere Einträge\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES_GERMAN_ENGLISH = {\n",
    "    #'zero_shot_to-english_english_1': {\n",
    "    #    'template': 'Please translate the following text from German to English: \\\"{text}\\\"',\n",
    "    #    'prompt_language': Language.GERMAN\n",
    "    #},\n",
    "    #'zero_shot_to-english_german_1': {\n",
    "    #    'template': 'Bitte übersetze diesen Text von Deutsch nach Englisch: \\\"{text}\\\"',\n",
    "    #    'prompt_language': Language.GERMAN\n",
    "    #},\n",
    "    'zero_shot_to-english_english_2': {\n",
    "        'template': 'Please translate the following text from German to English. Do not ask questions, just return the translation. This is the text: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.ENGLISH\n",
    "    },\n",
    "    'zero_shot_to-english_german_2': {\n",
    "        'template': 'Bitte übersetze diesen Text von Deutsch nach Englisch. Stelle keine Gegenfragen, sondern gib einfach die Übersetzung aus. Das ist der Text: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.GERMAN\n",
    "    },\n",
    "    # ...\n",
    "}"
   ],
   "id": "fbda21553c9aa77b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 4 Execute Pipeline"
   ],
   "id": "770e5c8f7b34ed45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T01:03:18.628332Z",
     "start_time": "2025-01-31T23:36:24.566755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translation_results = run_pipeline(data)\n",
    "print('Fertig mit der Pipeline. Speichere Ergebnisse...')\n",
    "translation_results.to_csv('translation_results.csv', sep=';')\n",
    "print('Pipeline abgeschlossen. Ergebnisse gespeichert.')"
   ],
   "id": "54362f91437fda00",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 474; n_ctx: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (576) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=568\n",
      "Prompt finished in (seconds):  1.92\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 30.90 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 49.45 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 27.02 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  31.48\n",
      "🏃 View run gemma/easy/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/fe50569bd3304bb89273f6aa4966ec75\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "Prompt finished in (seconds):  1.63\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 159.00 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.66 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  24.19\n",
      "🏃 View run gemma/easy/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/2dc50f17d2624a45b12f365a5d16816e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "Prompt finished in (seconds):  1.77\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 10.41 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 39.99 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 33.33 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 43.47 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  24.16\n",
      "🏃 View run gemma/easy/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/5c8e0953775e4206abcb7ff0f93281db\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "Prompt finished in (seconds):  2.18\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 164.38 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  24.35\n",
      "🏃 View run gemma/easy/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/cfd023b3ef644695b350f5df9b2c0e00\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 304; n_ctx: 364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (384) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=364\n",
      "Prompt finished in (seconds):  3.58\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 196.93 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  19.7\n",
      "🏃 View run gemma/news_gen/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/0ecf7f53f2294c7ba4ce8c6b80c0cc6a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "Prompt finished in (seconds):  6.59\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 196.24 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 65.53 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  21.47\n",
      "🏃 View run gemma/news_gen/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/adb25c4a0d9f4f80ae7bc97ef0c6c9bb\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "Prompt finished in (seconds):  1.29\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 175.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  19.01\n",
      "🏃 View run gemma/news_gen/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/ebafd0ca35f242a083b133a55f8b172d\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "Prompt finished in (seconds):  8.49\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.58 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 81.34 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  22.24\n",
      "🏃 View run gemma/news_gen/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/f551551fbdc14478a847245b9790b95d\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 484; n_ctx: 580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (608) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=580\n",
      "Prompt finished in (seconds):  1.78\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.66 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 69.87 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 73.97 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  22.71\n",
      "🏃 View run gemma/news_spec/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/5fb004218f8248fd800118c9fd12a8ba\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "Prompt finished in (seconds):  11.06\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 186.01 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  29.54\n",
      "🏃 View run gemma/news_spec/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/08590702b8e64078b31156ea8af9e6fb\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "Prompt finished in (seconds):  1.4\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 142.82 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.32 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.49 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 75.96 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  22.8\n",
      "🏃 View run gemma/news_spec/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/d474c391f7934a29bf21ac5cae62fce7\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "Prompt finished in (seconds):  2.84\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 172.17 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.32 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 73.02 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Metric Calculation in (seconds):  23.74\n",
      "🏃 View run gemma/news_spec/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/41c656d421c74d24af5e61232c0d7e0b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 498; n_ctx: 597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (608) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=597\n",
      "Prompt finished in (seconds):  1.75\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.28 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.50 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  23.12\n",
      "🏃 View run gemma/pop_science/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/39d397054d784190bf3b52ada381c774\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n",
      "Prompt finished in (seconds):  11.98\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 163.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  30.35\n",
      "🏃 View run gemma/pop_science/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/af1328f0c26c43c6bbea625e7281cb0d\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n",
      "Prompt finished in (seconds):  1.43\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 160.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.89 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  23.17\n",
      "🏃 View run gemma/pop_science/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/36d2a9a917cc4c8e9c8238e9f9555c70\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n",
      "Prompt finished in (seconds):  1.44\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 144.79 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.69 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Metric Calculation in (seconds):  23.33\n",
      "🏃 View run gemma/pop_science/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/a4357dc30305400991b07545ffebed88\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 1054; n_ctx: 1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (1280) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-ai/gemma-2b-it-GGUF erfolgreich geladen mit n_ctx=1264\n",
      "Prompt finished in (seconds):  24.88\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  56.21\n",
      "🏃 View run gemma/science/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/895795565739563094/runs/e606b7840813434598c51e50f7fe6c8e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/895795565739563094\n",
      "Prompt finished in (seconds):  21.85\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 199.96 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.89 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  51.65\n",
      "🏃 View run gemma/science/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/895795565739563094/runs/fa713fb84b1f4c60b3b2938df6965f2f\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/895795565739563094\n",
      "Prompt finished in (seconds):  3.11\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  36.64\n",
      "🏃 View run gemma/science/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/895795565739563094/runs/d91dc715b4534e618fadfcb565b8594b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/895795565739563094\n",
      "Prompt finished in (seconds):  25.97\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 199.95 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.49 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  53.56\n",
      "🏃 View run gemma/science/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/895795565739563094/runs/8c94a6c7381d41e9b46826e69128650f\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/895795565739563094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 542; n_ctx: 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (672) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=650\n",
      "Prompt finished in (seconds):  21.71\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 199.96 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 80.85 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  33.89\n",
      "🏃 View run llama32/easy/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/170611153105097398/runs/d25de20d17ab424590f143a4898940bc\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/170611153105097398\n",
      "Prompt finished in (seconds):  49.89\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 142.83 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  52.36\n",
      "🏃 View run llama32/easy/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/170611153105097398/runs/57995fec495a40a69c94af7802019854\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/170611153105097398\n",
      "Prompt finished in (seconds):  50.24\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 199.98 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  49.6\n",
      "🏃 View run llama32/easy/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/170611153105097398/runs/4eb995bf526840a996a59faea5c788e7\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/170611153105097398\n",
      "Prompt finished in (seconds):  20.69\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.59 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  31.78\n",
      "🏃 View run llama32/easy/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/170611153105097398/runs/d62ca082b58d4ebfae401b96da4aad0b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/170611153105097398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 354; n_ctx: 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (448) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=424\n",
      "Prompt finished in (seconds):  10.34\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 183.23 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.70 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  22.6\n",
      "🏃 View run llama32/news_gen/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/379656095678811037/runs/748ea00f6ff34f8d94f2010961159506\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/379656095678811037\n",
      "Prompt finished in (seconds):  12.29\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 164.24 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  22.03\n",
      "🏃 View run llama32/news_gen/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/379656095678811037/runs/a5cb410bb0424278968cb163a0046576\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/379656095678811037\n",
      "Prompt finished in (seconds):  13.05\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.61 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 82.97 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  21.97\n",
      "🏃 View run llama32/news_gen/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/379656095678811037/runs/c9253ce17e1d4ec59fe163a2f78e5e71\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/379656095678811037\n",
      "Prompt finished in (seconds):  32.05\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 27.77 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 43.47 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 37.03 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  29.88\n",
      "🏃 View run llama32/news_gen/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/379656095678811037/runs/60305836189749fa8d7857b3d11bcf34\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/379656095678811037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 556; n_ctx: 667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (672) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=667\n",
      "Prompt finished in (seconds):  17.64\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.42 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.82 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  31.7\n",
      "🏃 View run llama32/news_spec/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/194158564450517751/runs/9472f333743b465c8ae909143474d357\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/194158564450517751\n",
      "Prompt finished in (seconds):  50.38\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 187.08 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 75.24 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Metric Calculation in (seconds):  41.99\n",
      "🏃 View run llama32/news_spec/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/194158564450517751/runs/777fb16776d04d73a4270555eb65d53b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/194158564450517751\n",
      "Prompt finished in (seconds):  47.44\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 177.22 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.80 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Metric Calculation in (seconds):  47.2\n",
      "🏃 View run llama32/news_spec/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/194158564450517751/runs/3a0343b8604946b3aeb7a9597b575d6b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/194158564450517751\n",
      "Prompt finished in (seconds):  18.61\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.64 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  29.42\n",
      "🏃 View run llama32/news_spec/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/194158564450517751/runs/3b492c2ae0904effa80e59b4077d8f71\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/194158564450517751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 572; n_ctx: 686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (704) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=686\n",
      "Prompt finished in (seconds):  14.35\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 172.86 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  29.21\n",
      "🏃 View run llama32/pop_science/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/553009959054917340/runs/1abf47de2dea4e41a91399c0280ac09a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/553009959054917340\n",
      "Prompt finished in (seconds):  52.09\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 174.20 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.89 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 82.93 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 998.41it/s]\n",
      "Metric Calculation in (seconds):  46.99\n",
      "🏃 View run llama32/pop_science/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/553009959054917340/runs/131a0a3e1cad4cfc94b98e22a35d20d1\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/553009959054917340\n",
      "Prompt finished in (seconds):  25.2\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.60 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 78.62 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  32.92\n",
      "🏃 View run llama32/pop_science/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/553009959054917340/runs/d9877dd625784071961b108ca75bd82f\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/553009959054917340\n",
      "Prompt finished in (seconds):  23.03\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 163.64 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 79.75 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  31.49\n",
      "🏃 View run llama32/pop_science/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/553009959054917340/runs/6e16bac23e044e11a590b072438caf84\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/553009959054917340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 1110; n_ctx: 1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (1344) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF erfolgreich geladen mit n_ctx=1332\n",
      "Prompt finished in (seconds):  32.76\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 163.21 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 66.65 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  55.3\n",
      "🏃 View run llama32/science/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/248431796146488211/runs/2861e1ef75b747669e351d6ffa84b13b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/248431796146488211\n",
      "Prompt finished in (seconds):  17.57\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.30 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  42.85\n",
      "🏃 View run llama32/science/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/248431796146488211/runs/0cd32e62ad56438ab7a56a97aae40719\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/248431796146488211\n",
      "Prompt finished in (seconds):  44.4\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 160.12 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 80.83 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  61.38\n",
      "🏃 View run llama32/science/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/248431796146488211/runs/0fca38ff8c684960982b46f5b47c2203\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/248431796146488211\n",
      "Prompt finished in (seconds):  38.31\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 168.57 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Metric Calculation in (seconds):  55.43\n",
      "🏃 View run llama32/science/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/248431796146488211/runs/ac77f1a90d3c4c54adc64b4c25b58d4b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/248431796146488211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 542; n_ctx: 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (672) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=650\n",
      "Prompt finished in (seconds):  79.42\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 26.33 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 31.67 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 72.71 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  56.6\n",
      "🏃 View run llama31/easy/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/496790586212187793/runs/e1856aa2a47d4d2b857518b478b870ed\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/496790586212187793\n",
      "Prompt finished in (seconds):  83.32\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 141.91 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.08 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.63 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  54.63\n",
      "🏃 View run llama31/easy/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/496790586212187793/runs/6b87c944f37f44b88a7b6a62cdd98ad9\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/496790586212187793\n",
      "Prompt finished in (seconds):  77.15\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 164.33 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 85.79 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.50 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  44.19\n",
      "🏃 View run llama31/easy/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/496790586212187793/runs/eedac2a469ec49648060053791e2bde6\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/496790586212187793\n",
      "Prompt finished in (seconds):  80.54\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.67 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  46.37\n",
      "🏃 View run llama31/easy/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/496790586212187793/runs/53156960b778431097bebdd86a0b5991\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/496790586212187793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 354; n_ctx: 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (448) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=424\n",
      "Prompt finished in (seconds):  50.72\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.14 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.87 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 84.93 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Metric Calculation in (seconds):  31.7\n",
      "🏃 View run llama31/news_gen/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/378317835055917962/runs/5fd27969b0da4393969500fb82aa25d0\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/378317835055917962\n",
      "Prompt finished in (seconds):  53.7\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 173.63 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.87 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  31.59\n",
      "🏃 View run llama31/news_gen/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/378317835055917962/runs/83dcd7c80b8146329abfa20f41cc622e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/378317835055917962\n",
      "Prompt finished in (seconds):  48.15\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 157.07 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 59.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  29.99\n",
      "🏃 View run llama31/news_gen/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/378317835055917962/runs/99234eff5e7b45b49bbe26f8e06272a3\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/378317835055917962\n",
      "Prompt finished in (seconds):  52.6\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.64 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 62.48 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 80.40 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  30.36\n",
      "🏃 View run llama31/news_gen/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/378317835055917962/runs/3f019245a5fe4ef485d9f0cc8343b607\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/378317835055917962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 556; n_ctx: 667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (672) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=667\n",
      "Prompt finished in (seconds):  32.27\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  49.99\n",
      "🏃 View run llama31/news_spec/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/278262578605400461/runs/be3fce1e92a342aca1ba953c87586cc4\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/278262578605400461\n",
      "Prompt finished in (seconds):  84.86\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.61 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.91 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  50.72\n",
      "🏃 View run llama31/news_spec/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/278262578605400461/runs/f768628711ac45de825079607c83320c\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/278262578605400461\n",
      "Prompt finished in (seconds):  78.33\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  47.36\n",
      "🏃 View run llama31/news_spec/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/278262578605400461/runs/3a8a25090da94e988e761b7c6bd6c4ed\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/278262578605400461\n",
      "Prompt finished in (seconds):  81.18\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 165.44 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.30 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 72.28 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  43.62\n",
      "🏃 View run llama31/news_spec/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/278262578605400461/runs/c35ce4dc64ab4c5bb4860cbb4c3b3603\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/278262578605400461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 572; n_ctx: 686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (704) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=686\n",
      "Prompt finished in (seconds):  84.57\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 165.05 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.87 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 74.05 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  52.64\n",
      "🏃 View run llama31/pop_science/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/358136329887025901/runs/a7d01973ab7d4c6c81d5287c5dfd7dad\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/358136329887025901\n",
      "Prompt finished in (seconds):  87.47\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.69 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 55.54 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.41 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  53.11\n",
      "🏃 View run llama31/pop_science/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/358136329887025901/runs/46279fda567141c08f9748639c6ce06e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/358136329887025901\n",
      "Prompt finished in (seconds):  81.04\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.67 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.88 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 56.93 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 78.58 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  43.85\n",
      "🏃 View run llama31/pop_science/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/358136329887025901/runs/cb3ba2f0b6e74662a05dcb74dce1a51d\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/358136329887025901\n",
      "Prompt finished in (seconds):  84.25\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.61 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.89 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 58.81 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 81.37 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  47.08\n",
      "🏃 View run llama31/pop_science/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/358136329887025901/runs/d0236d4afb0c42b4b4962acef3286951\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/358136329887025901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=512\n",
      "estimated_max_tokens: 1110; n_ctx: 1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (1344) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF erfolgreich geladen mit n_ctx=1332\n",
      "Prompt finished in (seconds):  170.41\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 200.01 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 49.99 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 68.62 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Metric Calculation in (seconds):  157.16\n",
      "🏃 View run llama31/science/zero_shot_to-english_english_2 at: http://127.0.0.1:5000/#/experiments/958464439758456234/runs/4859e781a7ce4ec9a8c495cd9c1ccecd\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/958464439758456234\n",
      "Prompt finished in (seconds):  174.22\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 160.65 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.89 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 52.62 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 71.71 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Metric Calculation in (seconds):  113.09\n",
      "🏃 View run llama31/science/zero_shot_to-english_german_2 at: http://127.0.0.1:5000/#/experiments/958464439758456234/runs/97acb341e1f2404bbb950163f63d518f\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/958464439758456234\n",
      "Prompt finished in (seconds):  161.38\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 166.63 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 83.31 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 49.99 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 74.45 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Metric Calculation in (seconds):  124.31\n",
      "🏃 View run llama31/science/zero_shot_to-german_english_2 at: http://127.0.0.1:5000/#/experiments/958464439758456234/runs/f8cfc96b69b345af959d0b30223aa0bf\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/958464439758456234\n",
      "Prompt finished in (seconds):  166.71\n",
      "ERROR: You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "ERROR: C:\\Users\\DCA\\anaconda3\\envs\\softwarequality\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "ERROR:   warnings.warn(\n",
      "ERROR: \n",
      "ERROR: Generating test split: 0 examples [00:00, ? examples/s]\n",
      "ERROR: Generating test split: 1 examples [00:00, 175.80 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 90.87 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 49.99 examples/s]\n",
      "ERROR: \n",
      "ERROR: Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "ERROR: Map: 100%|##########| 1/1 [00:00<00:00, 76.90 examples/s]\n",
      "ERROR: Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "ERROR: \n",
      "ERROR:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "ERROR: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "Metric Calculation in (seconds):  131.49\n",
      "🏃 View run llama31/science/zero_shot_to-german_german_2 at: http://127.0.0.1:5000/#/experiments/958464439758456234/runs/98fd5f3834794a15aa73ac0c81c9329f\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/958464439758456234\n",
      "Fertig mit der Pipeline. Speichere Ergebnisse...\n",
      "Pipeline abgeschlossen. Ergebnisse gespeichert.\n"
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
