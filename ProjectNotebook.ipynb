{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jupyter Notebook for Project \"Comparison of LLM Prompting Techniques\"\n",
    "\n",
    "_Copyright 2025 Aldenkirchs & Reichert_\n",
    "\n",
    "_All code in this notebook is licensed under the same license as specified in the LICENSE file in the root directory of this project (see LICENSE)._"
   ],
   "id": "7817965d4d5e53b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.220015Z",
     "start_time": "2025-02-10T20:00:07.215989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import sacrebleu\n",
    "from llama_cpp import Llama\n",
    "import time\n",
    "from enum import Enum\n",
    "from rouge_score import rouge_scorer\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import gc\n"
   ],
   "id": "e3ba848b0dea6c4d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 Data Loading\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe."
   ],
   "id": "55f837b4b3d7b0c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.251058Z",
     "start_time": "2025-02-10T20:00:07.243282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_pickle('machine_translation.pkl')\n",
    "data"
   ],
   "id": "d0b66660e0495955",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity                                        text_german  \\\n",
       "0         easy  Felix hat es satt: St채ndig ist Mama unterwegs....   \n",
       "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
       "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
       "3  pop_science  Dass der Klimawandel die Hitzewellen in S체dasi...   \n",
       "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
       "\n",
       "                                        text_english  \n",
       "0  Felix is fed up: Mom is always on the go. But ...  \n",
       "1  The approximately 1,400 deployed officers have...  \n",
       "2  The head of state also has the right to appoin...  \n",
       "3  There is no question that climate change is in...  \n",
       "4  The DSA-110, situated at the Owens Valley Radi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german</th>\n",
       "      <th>text_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>Felix hat es satt: St채ndig ist Mama unterwegs....</td>\n",
       "      <td>Felix is fed up: Mom is always on the go. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>Die rund 1.400 eingesetzten Beamten haben demn...</td>\n",
       "      <td>The approximately 1,400 deployed officers have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>Der Staatschef hat zugleich aber das Recht, vo...</td>\n",
       "      <td>The head of state also has the right to appoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>Dass der Klimawandel die Hitzewellen in S체dasi...</td>\n",
       "      <td>There is no question that climate change is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Der DSA-110, der sich am Owens Valley Radio Ob...</td>\n",
       "      <td>The DSA-110, situated at the Owens Valley Radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.300823Z",
     "start_time": "2025-02-10T20:00:07.291380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['complexity'] = data['complexity']\n",
    "data_info['text_german_length'] = data['text_german'].str.len()\n",
    "data_info['text_english_length'] = data['text_english'].str.len()\n",
    "data_info"
   ],
   "id": "4578af3a4c1ceab2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity  text_german_length  text_english_length\n",
       "0         easy                 485                  415\n",
       "1     news_gen                 296                  280\n",
       "2    news_spec                 518                  484\n",
       "3  pop_science                 542                  521\n",
       "4      science                1003                  827"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german_length</th>\n",
       "      <th>text_english_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>485</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>296</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>518</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>542</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>1003</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here we collected some static variables and enums for improved maintainability and\n",
    "code reliability. The `ESTIMATED_TOKENS_BUFFER` and `MLFLOW_TRACKING_URI` can be configured based on individual preferences/ setup."
   ],
   "id": "ce284aeff759e0ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.348444Z",
     "start_time": "2025-02-10T20:00:07.344398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Language(Enum):\n",
    "    ENGLISH = 'English'\n",
    "    GERMAN = 'German'\n",
    "\n",
    "\n",
    "class Complexity(Enum):\n",
    "    EASY = 'easy'\n",
    "    NEWS_GEN = 'news_gen'\n",
    "    NEWS_SPEC = 'news_spec'\n",
    "    POP_SCIENCE = 'pop_science'\n",
    "    SCIENCE = 'science'\n",
    "\n",
    "\n",
    "ALL_COMPLEXITIES = list(Complexity)\n",
    "\n",
    "# this constant value is later used to calculate the estimated tokens and context size\n",
    "# -> it gets later multiplied by the token length of the prompt template + source text + reference text\n",
    "#       and should be > 1 but not too big\n",
    "# we identified 1.5 as a good heuristic\n",
    "ESTIMATED_TOKENS_BUFFER = 1.5\n",
    "\n",
    "# this is the default mlflow tracking uri and needs to be adjusted\n",
    "#    if mlflow is available under a different/ remote uri\n",
    "MLFLOW_TRACKING_URI = 'http://127.0.0.1:5000'"
   ],
   "id": "cec3e9fa6e97272f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 2 Model Loading\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp in case of problems can be found here:\n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python"
   ],
   "id": "bcb0a44c2b878f7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.458445Z",
     "start_time": "2025-02-10T20:00:07.454948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration of the models\n",
    "MODELS = {\n",
    "    'gemma': {\n",
    "        'repo_id': 'lmstudio-ai/gemma-2b-it-GGUF',\n",
    "        'filename': 'gemma-2b-it-q8_0.gguf',\n",
    "    },\n",
    "    'llama32': {\n",
    "        'repo_id': 'hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF',\n",
    "        'filename': 'llama-3.2-3b-instruct-q8_0.gguf',\n",
    "    },\n",
    "    'llama31': {\n",
    "        'repo_id': 'lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "        'filename': 'Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
    "    },\n",
    "    'aya23': {\n",
    "        'repo_id': 'bartowski/aya-23-35B-GGUF',\n",
    "        'filename': 'aya-23-35B-Q5_K_M.gguf',\n",
    "    },\n",
    "}"
   ],
   "id": "b523e2b238ca1806",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.522372Z",
     "start_time": "2025-02-10T20:00:07.517319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_llama_model(repo_id, filename, n_ctx=None):\n",
    "    \"\"\"\n",
    "    Loads and creates the Llama model from the specified repository and file.\n",
    "\n",
    "    Args:\n",
    "        repo_id: repository ID of the model.\n",
    "        filename: filename of the model.\n",
    "        n_ctx: context window size for the model. Defaults to 512 if None.\n",
    "\n",
    "    Returns:\n",
    "        The loaded Llama model, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if n_ctx is None:\n",
    "            # default of llama_cpp\n",
    "            n_ctx = 512\n",
    "        if repo_id is not None and filename is not None:\n",
    "            model = Llama.from_pretrained(\n",
    "                repo_id=repo_id,\n",
    "                filename=filename,\n",
    "                n_ctx=n_ctx,\n",
    "                # these parameters can be set individually based on the running system\n",
    "                #n_gpu_layers=n_gpu_layers,\n",
    "                #n_threads=8,\n",
    "                verbose=False,\n",
    "            )\n",
    "            print(f\"Model {repo_id} successfully loaded with n_ctx={n_ctx}\")\n",
    "            return model\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred when loading the model from file: {filename}: {e}\")\n",
    "        return None"
   ],
   "id": "d77089c8b718fea5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "50eef963fb4c35d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3 Pipeline\n",
    "\n",
    "### 3.1 Model Interaction"
   ],
   "id": "6abec46ebf71a32b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.589202Z",
     "start_time": "2025-02-10T20:00:07.584777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(model, prompt, reference_translation):\n",
    "    \"\"\"\n",
    "    Translates the given prompt using the provided model.\n",
    "    estimates the needed max_tokens based on the lengths of the prompt and the reference translation.\n",
    "\n",
    "    Args:\n",
    "        model: translation model to be used.\n",
    "        prompt: text to be translated.\n",
    "        reference_translation: reference translation used to estimate max_tokens.\n",
    "\n",
    "    Returns:\n",
    "        The translated text.\n",
    "    \"\"\"\n",
    "    # we estimate the needed max_tokens based on the tokenized prompt and reference_translation\n",
    "    token_length_ref = len(model.tokenize(reference_translation.encode('utf-8')))\n",
    "    token_length_prompt = len(model.tokenize(prompt.encode('utf-8')))\n",
    "    # the model should not need more tokens than this\n",
    "    estimated_max_tokens = (token_length_prompt + token_length_ref) * ESTIMATED_TOKENS_BUFFER\n",
    "\n",
    "    response = model(prompt, max_tokens=estimated_max_tokens, echo=False)\n",
    "    return response['choices'][0]['text']"
   ],
   "id": "1ce54c91795c7a3d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Metrics Calculation\n",
    "\n",
    "These two code cells calculate the metric scores based on the hypothesis and reference translation. Here we also integrated MetricX into our project (\n",
    "[GitHub repository to MetricX](https://github.com/google-research/metricx))"
   ],
   "id": "9e72246e3b127e4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.653283Z",
     "start_time": "2025-02-10T20:00:07.646837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_translation(source, reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of a translation (hypothesis) against a reference translation,\n",
    "    calculating BLEU, chrF, MetricX, and RougeL scores.\n",
    "\n",
    "    Args:\n",
    "        source: source text.\n",
    "        reference: reference translation.\n",
    "        hypothesis: hypothesis.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the BLEU, chrF, RougeL, and MetricX scores.  BLEU, chrF, and RougeL\n",
    "        are scaled to be between 0 and 100. MetricX will be -1 if it cannot be calculated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that BLEU and chrF Scores can only be between 0 and 1\n",
    "    #   but sacreblue returns floats between 0 and 100\n",
    "    bleu_score = sacrebleu.corpus_bleu([hypothesis], [[reference]]).score\n",
    "    chrf_score = sacrebleu.corpus_chrf([hypothesis], [[reference]]).score\n",
    "\n",
    "    metricx_score = calculate_metricx_score(source, reference, hypothesis)\n",
    "    if metricx_score is None:\n",
    "        metricx_score = -1\n",
    "\n",
    "    rougel_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rougel_score = rougel_scorer.score(reference, hypothesis)\n",
    "\n",
    "    return {'BLEU': bleu_score,\n",
    "            'chrF': chrf_score,\n",
    "            # we also edited the rougeL score to lie between 0 and 100 (to be similar to BLEU and chrF)\n",
    "            'rougeL': (rougel_score['rougeL'].fmeasure * 100),\n",
    "            'MetricX': metricx_score}\n"
   ],
   "id": "5d0726d94411242c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.618696Z",
     "start_time": "2025-02-10T20:00:07.611575Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 22,
   "source": [
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    '''\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    '''\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = './temp_input.jsonl'\n",
    "    output_file = './temp_output.jsonl'\n",
    "    # this is the model that is used for evaluation\n",
    "    model = 'google/metricx-24-hybrid-large-v2p6-bfloat16'\n",
    "\n",
    "    tmp_data = [{'id': '1', 'source': source, 'reference': reference, 'hypothesis': hypothesis}]\n",
    "    try:\n",
    "        with open(input_file, 'w', encoding='utf-8') as f:\n",
    "            for entry in tmp_data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        command = [\n",
    "            'python', '-m', 'metricx24.predict',\n",
    "            '--tokenizer', 'google/mt5-xl',\n",
    "            '--model_name_or_path', model,\n",
    "            '--max_input_length', '1536',\n",
    "            '--batch_size', '1',\n",
    "            '--input_file', input_file,\n",
    "            '--output_file', output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        #for line in process.stdout:\n",
    "        #    print(line, end='')\n",
    "        #for line in process.stderr:\n",
    "        #    print(f'ERROR: {line}', end='')\n",
    "\n",
    "        # wait for the metric calculation process to terminate\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f'Error executing metricx24. Return code: {process.returncode}')\n",
    "            return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get('prediction'))\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print('Error parsing the output file.')\n",
    "                    return None\n",
    "\n",
    "        return None  # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass  #If the files don't exist for some reason, the error is caught\n",
    "\n",
    "\n"
   ],
   "id": "29a087ab8c8c4970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Logging to MLFLow",
   "id": "154ad4fd908110a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.722735Z",
     "start_time": "2025-02-10T20:00:07.717359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_to_mlflow(experiment_name, template_name, metrics, prompt_type, model_name, complexity, target_language,\n",
    "                  tmp_result,\n",
    "                  prompt_language):\n",
    "    \"\"\"\n",
    "    Logs results of a run to MLflow.\n",
    "    creates the respective experiment if it does not already exist\n",
    "\n",
    "    Args:\n",
    "        experiment_name: name of the MLflow experiment.\n",
    "        template_name: name of the prompt template used.\n",
    "        metrics: dictionary of metrics to log.\n",
    "        prompt_type: prompting technique that was used.\n",
    "        model_name: name of the model.\n",
    "        complexity: complexity level of the source text.\n",
    "        target_language: target language of the translation.\n",
    "        tmp_result: Pandas DataFrame containing temporary results.\n",
    "        prompt_language: language of the prompt.\n",
    "    \"\"\"\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    if experiment:\n",
    "        if experiment.lifecycle_stage == 'deleted':\n",
    "            mlflow.tracking.MlflowClient().restore_experiment(experiment.experiment_id)\n",
    "    else:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f'{model_name}/{complexity}/{template_name}'):\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('complexity', complexity)\n",
    "        mlflow.log_param('prompt_type', prompt_type)\n",
    "        mlflow.log_param('target_language', target_language)\n",
    "        mlflow.log_param('prompt_language', prompt_language)\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        tmp_result.to_json('tmp_results.json', index=False)\n",
    "        mlflow.log_artifact('tmp_results.json')\n",
    "        mlflow.end_run()\n"
   ],
   "id": "a1cf3e58d221a648",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4 Pipeline Composition\n",
    "\n",
    "This is the main part of our pipeline where all the code snippets from above come together."
   ],
   "id": "bc742452c43246a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.751475Z",
     "start_time": "2025-02-10T20:00:07.736608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline(texts):\n",
    "    \"\"\"\n",
    "    Runs the translation pipeline for a given set of texts, iterating through the different MODELS (from above),\n",
    "    complexities, and PROMPT_TEMPLATES. Also logs the results to MLflow and returns them as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        texts: Pandas DataFrame containing the source and reference texts, as well as the\n",
    "            complexity level for each text.\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame containing the results of all translation runs.\n",
    "    \"\"\"\n",
    "    # this is the result Dataframe where all runs are stored\n",
    "    results = pd.DataFrame(\n",
    "        columns=['model', 'complexity', 'prompt_type', 'prompt', 'source_text', 'hypothesis', 'reference', 'metrics',\n",
    "                 'prompt_language'])\n",
    "\n",
    "    # this is just for mlflow and can be changed individually\n",
    "    mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "    for model_name, model_config in MODELS.items():\n",
    "        for _, row in texts.iterrows():\n",
    "            model = createModel(model_config, row)\n",
    "            complexity_enum = next(c for c in Complexity if c.value == row['complexity'])\n",
    "\n",
    "            # translations German -> English\n",
    "            for template_name, template_data in PROMPT_TEMPLATES_GERMAN_ENGLISH.items():\n",
    "                if pd.notna(row['text_german']) and complexity_enum in template_data['complexities']:\n",
    "                    results = execute_mlflow_run(template_name, complexity_enum.value, model, model_name,\n",
    "                                                 Language.ENGLISH, results,\n",
    "                                                 row['text_german'], row['text_english'], template_data)\n",
    "\n",
    "            # translations English -> German\n",
    "            for template_name, template_data in PROMPT_TEMPLATES_ENGLISH_GERMAN.items():\n",
    "                if pd.notna(row['text_english']) and complexity_enum in template_data['complexities']:\n",
    "                    results = execute_mlflow_run(template_name, complexity_enum.value, model, model_name,\n",
    "                                                 Language.GERMAN, results,\n",
    "                                                 row['text_english'], row['text_german'], template_data)\n",
    "\n",
    "            # we dont need the model anymore so we delete it\n",
    "            del model\n",
    "            gc.collect()\n",
    "    return results\n",
    "\n",
    "\n",
    "def createModel(model_config, row):\n",
    "    \"\"\"\n",
    "    Creates the desired model with a context window (n_ctx) that is estimated\n",
    "    based on the token length of the prompt, source and reference text.\n",
    "\n",
    "    Args:\n",
    "        model_config: configuration for the language model.\n",
    "        row: row from the input DataFrame containing the source and reference texts (needed for token estimation).\n",
    "\n",
    "    Returns:\n",
    "        The created language model.\n",
    "    \"\"\"\n",
    "    # at first we just use the dummyModel for the tokenization of text\n",
    "    dummyModel = create_llama_model(model_config['repo_id'], model_config['filename'])\n",
    "\n",
    "    # then we determine the minimal tokens needed for a translation (prompt + source text + reference text)\n",
    "    combined_text = f\"{row['text_german']} {row['text_english']}\"\n",
    "    text_tokens = len(dummyModel.tokenize(combined_text.encode('utf-8')))\n",
    "    # we want to tokenize the longest template/ prompt\n",
    "    max_promp_template = max(\n",
    "        (t['template'] for d in (PROMPT_TEMPLATES_GERMAN_ENGLISH, PROMPT_TEMPLATES_ENGLISH_GERMAN) for t in d.values()),\n",
    "        key=len)\n",
    "    prompt_tokens = len(dummyModel.tokenize(max_promp_template.encode('utf-8')))\n",
    "\n",
    "    # now we delete the dummyModel to free up memory\n",
    "    del dummyModel\n",
    "    gc.collect()\n",
    "\n",
    "    # and then create the final model based on the estimated_max_tokens\n",
    "    estimated_max_tokens = (text_tokens + prompt_tokens) * ESTIMATED_TOKENS_BUFFER\n",
    "    n_ctx = int(estimated_max_tokens * 1.1)\n",
    "    print(f\"estimated_max_tokens: {estimated_max_tokens}; n_ctx: {n_ctx}\")\n",
    "    model = create_llama_model(model_config['repo_id'], model_config['filename'], n_ctx=n_ctx)\n",
    "    return model\n",
    "\n",
    "\n",
    "def execute_mlflow_run(template_name, complexity, model, model_name, target_language: Language, results, source_text,\n",
    "                       reference_text, template_data):\n",
    "    \"\"\"\n",
    "    Executes a single translation run, including prompt creation, translation, evaluation, and logging to MLflow.\n",
    "\n",
    "    Args:\n",
    "        template_name: name of the prompt template used.\n",
    "        complexity: complexity level of the prompt.\n",
    "        model: language model used for translation.\n",
    "        model_name: name of the model.\n",
    "        target_language: target language of the translation.\n",
    "        results: Pandas DataFrame to store the results.\n",
    "        source_text: source text to be translated.\n",
    "        reference_text: reference translation.\n",
    "        template_data: data associated with the prompt template.\n",
    "\n",
    "    Returns:\n",
    "        the updated results DataFrame.\n",
    "    \"\"\"\n",
    "    # this is the actual composition of the prompt where '{text}' gets replaced with the source text\n",
    "    prompt = template_data['template'].format(text=source_text)\n",
    "\n",
    "    start_time_translation = time.time()\n",
    "    hypothesis = translate(model, prompt, reference_text)\n",
    "    end_time_translation = time.time()\n",
    "    print('Prompt finished in (seconds): ', round(end_time_translation - start_time_translation, 2))\n",
    "\n",
    "    metrics = evaluate_translation(source=source_text, reference=reference_text, hypothesis=hypothesis)\n",
    "    print('Metric Calculation finished in (seconds): ', round(time.time() - end_time_translation, 2))\n",
    "\n",
    "    prompt_language = template_data['prompt_language']\n",
    "    prompt_type = template_data['prompt_type']\n",
    "    tmp_result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'complexity': complexity,\n",
    "        'prompt_type': prompt_type,\n",
    "        'prompt': prompt,\n",
    "        'source_text': source_text,\n",
    "        'hypothesis': hypothesis,\n",
    "        'reference_text': reference_text,\n",
    "        'metrics': metrics,\n",
    "        'prompt_language': prompt_language.value  # .value for the string value\n",
    "    }])\n",
    "\n",
    "    experiment_name = f'{model_name}_{complexity}'\n",
    "\n",
    "    log_to_mlflow(experiment_name, template_name, metrics, prompt_type, model_name, complexity, target_language.value,\n",
    "                  tmp_result,\n",
    "                  prompt_language.value)\n",
    "\n",
    "    # add tmp_results Dataframe to overall results\n",
    "    results = pd.concat([\n",
    "        results,\n",
    "        tmp_result\n",
    "    ], ignore_index=True)\n",
    "    return results"
   ],
   "id": "602ba01d2e9660f7",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.5 Prompt Composition\n",
    "\n",
    "In this cell, the prompt_templates have to be defined. We provided two short examples although the collection of our templates and prompts can be found in the `prompt_templates_few_shot.ipynb` and `prompt_templates_zero_shot.ipynb` notebooks in this project.\n"
   ],
   "id": "8ab14fdc0cb769b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T20:00:07.771226Z",
     "start_time": "2025-02-10T20:00:07.764563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example for only using specific complexities: 'complexities': [Complexity.EASY, Complexity.NEWS_GEN],\n",
    "\n",
    "PROMPT_TEMPLATES_ENGLISH_GERMAN = {\n",
    "    'zero_shot_to-de_en_1': {\n",
    "        'template': 'Please translate the following text from English to German: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.ENGLISH,\n",
    "        'prompt_type': 'zero_shot',\n",
    "        'complexities': ALL_COMPLEXITIES\n",
    "    },\n",
    "    'zero_shot_to-de_de_1': {\n",
    "        'template': 'Bitte 체bersetze diesen Text von Englisch nach Deutsch: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.GERMAN,\n",
    "        'prompt_type': 'zero_shot',\n",
    "        'complexities': ALL_COMPLEXITIES\n",
    "    },\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES_GERMAN_ENGLISH = {\n",
    "    'zero_shot_to-en_en_1': {\n",
    "        'template': 'Please translate the following text from German to English: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.GERMAN,\n",
    "        'prompt_type': 'zero_shot_format',\n",
    "        'complexities': ALL_COMPLEXITIES\n",
    "    },\n",
    "    'zero_shot_to-en_de_1': {\n",
    "        'template': 'Bitte 체bersetze diesen Text von Deutsch nach Englisch: \\\"{text}\\\"',\n",
    "        'prompt_language': Language.GERMAN,\n",
    "        'prompt_type': 'zero_shot',\n",
    "        'complexities': ALL_COMPLEXITIES\n",
    "    },\n",
    "}"
   ],
   "id": "571cfff1360e09ef",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 4 Execute Pipeline\n",
    "\n",
    "Here, we just execute the pipeline. Depending on the number of prompt_templates and complexities, this can take a long time. The prompts usually take from 20 seconds on the small gemma model to 15 minutes on the big aya-23 model (for one single model output). The metric calculation should take about 30 - 60 seconds."
   ],
   "id": "b226050c15cbe2d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "translation_results = run_pipeline(data)\n",
    "translation_results.to_csv('translation_results.csv', sep=';')\n",
    "print('Pipeline abgeschlossen. Ergebnisse gespeichert.')"
   ],
   "id": "c7f5f9399827a4e8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
