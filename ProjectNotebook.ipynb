{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jupyter Notebook for Project \"Comparison of LLM Prompting Techniques\"",
   "id": "962e2691489cf6fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:15.038041Z",
     "start_time": "2025-01-21T10:46:15.028924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import sacrebleu\n",
    "from sacrebleu import corpus_bleu\n",
    "from llama_cpp import Llama\n",
    "import time\n"
   ],
   "id": "e2398ca7b5ff8ed5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 Data Loading\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe."
   ],
   "id": "fee57aff9aaf29b2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:15.219170Z",
     "start_time": "2025-01-21T10:46:15.123579Z"
    }
   },
   "source": [
    "data = pd.read_pickle('machine_translation.pkl')\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity                                        text_german  \\\n",
       "0         easy  Felix hat es satt: St√§ndig ist Mama unterwegs....   \n",
       "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
       "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
       "3  pop_science  Dass der Klimawandel die Hitzewellen in S√ºdasi...   \n",
       "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
       "\n",
       "                                        text_english  \n",
       "0  Felix is fed up: Mom is always on the go. But ...  \n",
       "1  The approximately 1,400 deployed officers have...  \n",
       "2  The head of state also has the right to appoin...  \n",
       "3  There is no question that climate change is in...  \n",
       "4  The DSA-110, situated at the Owens Valley Radi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german</th>\n",
       "      <th>text_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>Felix hat es satt: St√§ndig ist Mama unterwegs....</td>\n",
       "      <td>Felix is fed up: Mom is always on the go. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>Die rund 1.400 eingesetzten Beamten haben demn...</td>\n",
       "      <td>The approximately 1,400 deployed officers have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>Der Staatschef hat zugleich aber das Recht, vo...</td>\n",
       "      <td>The head of state also has the right to appoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>Dass der Klimawandel die Hitzewellen in S√ºdasi...</td>\n",
       "      <td>There is no question that climate change is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Der DSA-110, der sich am Owens Valley Radio Ob...</td>\n",
       "      <td>The DSA-110, situated at the Owens Valley Radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:15.308442Z",
     "start_time": "2025-01-21T10:46:15.288591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['complexity'] = data['complexity']\n",
    "data_info['text_german_length'] = data['text_german'].str.len()\n",
    "data_info['text_english_length'] = data['text_english'].str.len()\n",
    "data_info"
   ],
   "id": "7ff3eb7d21b8b721",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity  text_german_length  text_english_length\n",
       "0         easy                 485                  415\n",
       "1     news_gen                 296                  280\n",
       "2    news_spec                 518                  484\n",
       "3  pop_science                 542                  521\n",
       "4      science                1003                  827"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german_length</th>\n",
       "      <th>text_english_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>485</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>296</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>518</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>542</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>1003</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 2 Model Loading\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp:\n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python"
   ],
   "id": "bd9f6432346f6748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.215551Z",
     "start_time": "2025-01-21T10:46:15.407387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modelle laden\n",
    "gemma = Llama.from_pretrained(\n",
    "    repo_id='lmstudio-ai/gemma-2b-it-GGUF',\n",
    "    filename='gemma-2b-it-q8_0.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama32 = Llama.from_pretrained(\n",
    "    repo_id='hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF',\n",
    "    filename='llama-3.2-3b-instruct-q8_0.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama31 = Llama.from_pretrained(\n",
    "    repo_id='lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "    filename='Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "aya23 = Llama.from_pretrained(\n",
    "    repo_id='bartowski/aya-23-35B-GGUF',\n",
    "    filename='aya-23-35B-Q5_K_M.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    'gemma': gemma,\n",
    "    #'llama32': llama32,\n",
    "    #'llama31': llama31,\n",
    "    #'aya23': aya23,\n",
    "}"
   ],
   "id": "27d6d6528b3dcb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "62ac1a6e2dbdae96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Pipeline",
   "id": "8e5396fec902551e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Prompt Composition",
   "id": "4dba8b573c0379b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.245493Z",
     "start_time": "2025-01-21T10:46:18.243435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Beide Richtungen abbilden: English <-> German\n",
    "# TODO: Verschiedene Prompt Arten: zero-shot, few-shot und verschiedene Variationen reinbringen\n",
    "\n",
    "PROMPT_TEMPLATES_ENGLISH_GERMAN = {\n",
    "    'zero_shot_to-german_english_1': 'Please translate the following text from English to German: \\\"{text}\\\"',\n",
    "    'zero_shot_to-german_german_1': 'Bitte √ºbersetze den folgenden Text von Englisch nach Deutsch: {text}',\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from English to German.\n",
    "    #Here are some examples:\n",
    "    #- English: \"Hello\" -> German: \"Hallo\"\n",
    "    #- English: \"Goodbye\" -> German: \"Auf Wiedersehen\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte √ºbersetze einen Text von Englisch nach Deutsch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Englisch: \"Hello\" -> Deutsch: \"Hallo\"\n",
    "    #- Englisch: \"Goodbye\" -> Deutsch: \"Auf Wiedersehen\"\n",
    "    #Jetzt √ºbersetze diesen Text: {text}\"\"\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES_GERMAN_ENGLISH = {\n",
    "    'zero_shot_to-english_englisch_1': 'Please translate the following text from German to English: \\\"{text}\\\"',\n",
    "    'zero_shot-to-english_german_1': 'Bitte √ºbersetze den folgenden Text von Deutsch nach Englisch: {text}',\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from German to English.\n",
    "    #Here are some examples:\n",
    "    #- German: \"Hallo\" -> English: \"Hello\"\n",
    "    #- German: \"Auf Wiedersehen\" -> English: \"Goodbye\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte √ºbersetze einen Text von Deutsch nach Englisch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Deutsch: \"Hallo\" -> Englisch: \"Hello\"\n",
    "    #- Deutsch: \"Auf Wiedersehen\" -> Englisch: \"Goodbye\"\n",
    "    #Jetzt √ºbersetze diesen Text: {text}\"\"\",\n",
    "}"
   ],
   "id": "e70812e535e26c4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Model Interaction",
   "id": "cb00365dd1aad494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.331030Z",
     "start_time": "2025-01-21T10:46:18.328483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(model, prompt, reference_translation):\n",
    "    # we estimate the needed max_tokens based on the tokenized prompt and reference_translation\n",
    "    token_length_ref = len(model.tokenize(reference_translation.encode('utf-8')))\n",
    "    token_length_prompt = len(model.tokenize(prompt.encode('utf-8')))\n",
    "    # the model should not need more tokens than this\n",
    "    estimated_max_tokens = (token_length_prompt + token_length_ref) * 1.5\n",
    "\n",
    "    response = model(prompt, max_tokens=estimated_max_tokens, echo=False)\n",
    "    print(response)\n",
    "    return response['choices'][0]['text']"
   ],
   "id": "50fdfbda377216c8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.383075Z",
     "start_time": "2025-01-21T10:46:18.380875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt3 = 'Please translate the following text into German. Begin the translation with <<< and end it with >>>: The approximately 1,400 deployed officers have therefore arrested six suspected pickpockets at the start of the carnival and are now also investigating several cases of bodily harm and sexual offenses. Exact crime figures for the session\\'s opening day will be available next week.'\n",
    "\n",
    "# response = gemma.create_completion(prompt3, echo=False, max_tokens=200)\n",
    "\n",
    "# print(response)\n"
   ],
   "id": "49d48844fe5c7b16",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.467987Z",
     "start_time": "2025-01-21T10:46:18.460284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = 'Der Staatschef hat zugleich aber das Recht, vorl√§ufig Minister w√§hrend mindestens zehn Tage langen Sitzungspausen des Senats einzusetzen. Das soll die Handlungsf√§higkeit der Regierung gew√§hrleisten. Die so ernannten Minister m√ºssen dann bis Ende der Sitzungsperiode vom Senat best√§tigt werden, um weiter im Amt zu bleiben.\\n\\nDie Republikaner sicherten sich bei der Wahl eine Mehrheit im Senat mit mindestens 53 der 100 Sitze. Die Demokraten k√∂nnten aber das Ernennungsverfahren in den zust√§ndigen Aussch√ºssen verz√∂gern.'\n",
    "\n",
    "prompt2 = 'The head of state also has the right to appoint interim ministers during Senate recesses lasting at least ten days. This is to ensure the government\\'s ability to function. The ministers appointed in this manner must be confirmed by the Senate by the end of the session period to remain in office.\\n\\nThe Republicans secured a majority in the Senate with at least 53 of the 100 seats in the election. However, the Democrats could delay the appointment process in the relevant committees.'\n",
    "\n",
    "prompt3 = 'The approximately 1,400 deployed officers have therefore arrested six suspected pickpockets at the start of the carnival and are now also investigating several cases of bodily harm and sexual offenses. Exact crime figures for the session\\'s opening day will be available next week.'\n",
    "\n",
    "prompt4 = 'Die rund 1.400 eingesetzten Beamten haben demnach beim Start in den Karneval sechs mutma√üliche Taschendiebe festgenommen und ermitteln nun zudem wegen mehreren F√§llen von K√∂rperverletzungen und Sexualdelikten. Genaue Zahlen zur Kriminalit√§t am Sessionsauftakt soll es in der n√§chsten Woche geben.'\n",
    "\n",
    "\n",
    "print(len(llama31.tokenize(prompt2.encode('utf-8'))))\n",
    "print(len(llama31.tokenize(prompt.encode('utf-8'))))\n",
    "print(len(llama31.tokenize(prompt3.encode('utf-8'))))\n",
    "print(len(llama31.tokenize(prompt4.encode('utf-8'))))\n",
    "# response = llama31.create_completion(prompt, echo=False, max_tokens=400)\n",
    "# print(response)"
   ],
   "id": "f5193cb323a71aed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "145\n",
      "52\n",
      "86\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.524802Z",
     "start_time": "2025-01-21T10:46:18.520231Z"
    }
   },
   "cell_type": "code",
   "source": "{'id': 'cmpl-6d176ba7-9ab4-4fec-8522-25321557636c', 'object': 'text_completion', 'created': 1737399536, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/./Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf', 'choices': [{'text': '\"\\n\\nHere is the translation of the text from German to English:\\n\\n\"The head of state has the right, however, to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions. This is to ensure the government\\'s operational ability. The ministers thus appointed must then be confirmed by the Senate by the end of the legislative period in order to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\"\\n\\nLet me know if you need any further assistance! \\n(Translation is provided in a neutral tone and does not imply any particular interpretation or opinion.) \\n\\nI have translated the text into English, maintaining a neutral tone and avoiding any interpretation or opinion. If you require any further assistance or have any specific requests, please feel free to ask! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. Ministers thus appointed must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\\n\\nPlease let me know if you have any further requests or need any additional assistance! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. The ministers appointed in this way must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured a majority in', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 156, 'completion_tokens': 356, 'total_tokens': 512}}",
   "id": "88899180bca30d13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-6d176ba7-9ab4-4fec-8522-25321557636c',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1737399536,\n",
       " 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/./Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
       " 'choices': [{'text': '\"\\n\\nHere is the translation of the text from German to English:\\n\\n\"The head of state has the right, however, to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions. This is to ensure the government\\'s operational ability. The ministers thus appointed must then be confirmed by the Senate by the end of the legislative period in order to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\"\\n\\nLet me know if you need any further assistance! \\n(Translation is provided in a neutral tone and does not imply any particular interpretation or opinion.) \\n\\nI have translated the text into English, maintaining a neutral tone and avoiding any interpretation or opinion. If you require any further assistance or have any specific requests, please feel free to ask! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. Ministers thus appointed must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\\n\\nPlease let me know if you have any further requests or need any additional assistance! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. The ministers appointed in this way must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured a majority in',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 156,\n",
       "  'completion_tokens': 356,\n",
       "  'total_tokens': 512}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Metrics Calculation\n",
    "[GitHub Repo to MetricX](https://github.com/google-research/metricx)"
   ],
   "id": "23525c106ed337fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.560090Z",
     "start_time": "2025-01-21T10:46:18.556522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    '''\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    '''\n",
    "\n",
    "    data = [{'id': '1', 'source': source, 'reference': reference, 'hypothesis': hypothesis}]\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = './temp_input.jsonl'\n",
    "    output_file = './temp_output.jsonl'\n",
    "    model = 'google/metricx-24-hybrid-large-v2p6-bfloat16'\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'w', encoding='utf-8') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        command = [\n",
    "            'python', '-m', 'metricx24.predict',\n",
    "            '--tokenizer', 'google/mt5-xl',\n",
    "            '--model_name_or_path', model,\n",
    "            '--max_input_length', '1536',\n",
    "            '--batch_size', '1',\n",
    "            '--input_file', input_file,\n",
    "            '--output_file', output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        # for line in process.stdout:\n",
    "        #     print(line, end='')\n",
    "        # for line in process.stderr:\n",
    "        #     print(f'ERROR: {line}', end='')\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f'Error executing metricx24. Return code: {process.returncode}')\n",
    "            return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get('prediction'))  # Ensure that 'score' exists\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print('Error parsing the output file.')\n",
    "                    return None\n",
    "\n",
    "        return None  # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass  #If the files don't exist for some reason, the error is caught\n"
   ],
   "id": "4a5ab47796005682",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.610024Z",
     "start_time": "2025-01-21T10:46:18.607646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example call\n",
    "source_text = 'I am learning Python for Machine Learning.'\n",
    "reference_text = 'I am learning Python for machine learning.'\n",
    "hypothesis_text = \"I'm studying Python for machine learning.\"\n",
    "\n",
    "# score = calculate_metricx_score(source_text, reference_text, hypothesis_text)\n",
    "#\n",
    "# if score is not None:\n",
    "#     print(f'The calculated score is: {score}')\n",
    "# else:\n",
    "#     print('The score calculation failed.')"
   ],
   "id": "be6a0a27f7ff0d5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.644956Z",
     "start_time": "2025-01-21T10:46:18.642386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_translation(source, reference, hypothesis):\n",
    "    #TODO: ich wei√ü nicht, ob wir die Methoden von sacrebleu richtig benutzen\n",
    "    #   oder ob wir die Strings vielleicht noch in einzelne S√§tze splitten m√ºssen\n",
    "    bleu_score = corpus_bleu([hypothesis], [[reference]]).score\n",
    "    chrf_score = sacrebleu.corpus_chrf([hypothesis], [[reference]]).score\n",
    "    metricx_score = calculate_metricx_score(source, reference, hypothesis)\n",
    "    return {'BLEU': bleu_score,\n",
    "            'chrF': chrf_score,\n",
    "            'MetricX': metricx_score}"
   ],
   "id": "68b3d386c2d373aa",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Logging to MLFLow",
   "id": "3743c34bde1b7d77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.687879Z",
     "start_time": "2025-01-21T10:46:18.682170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, tmp_result):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    if experiment:\n",
    "        if experiment.lifecycle_stage == 'deleted':\n",
    "            mlflow.tracking.MlflowClient().restore_experiment(experiment.experiment_id)\n",
    "            #mlflow.delete_experiment(experiment.experiment_id)\n",
    "    else:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f'{model_name}/{complexity}/{prompt_type}'):\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('complexity', complexity)\n",
    "        mlflow.log_param('prompt_type', prompt_type)\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        tmp_result.to_json('tmp_results.json', index=False)\n",
    "        mlflow.log_artifact('tmp_results.json')\n",
    "        mlflow.end_run()\n"
   ],
   "id": "770d3703528c2548",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 Pipeline Composition",
   "id": "36fde1001499c4c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T10:46:18.718756Z",
     "start_time": "2025-01-21T10:46:18.715509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline(texts):\n",
    "    results = pd.DataFrame(\n",
    "        columns=['model', 'complexity', 'prompt_type', 'prompt', 'source_text', 'hypothesis', 'reference', 'metrics'])\n",
    "    mlflow.set_tracking_uri(uri='http://127.0.0.1:5000')\n",
    "\n",
    "    for model_name, model in MODELS.items():\n",
    "        for _, row in texts.iterrows():\n",
    "\n",
    "            # √úbersetzung Deutsch -> Englisch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_GERMAN_ENGLISH.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_german']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, results,\n",
    "                                                 row['text_german'], row['text_english'], template)\n",
    "\n",
    "            # √úbersetzung Englisch -> Deutsch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_ENGLISH_GERMAN.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_english']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, results,\n",
    "                                                 row['text_english'], row['text_german'], template)\n",
    "\n",
    "    # results.to_csv('results.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "def execute_mlflow_run(complexity, model, model_name, prompt_type, results, source_text, reference_text, template):\n",
    "    prompt = template.format(text=source_text)\n",
    "\n",
    "    start_time_translation = time.time()\n",
    "    hypothesis = translate(model, prompt, reference_text)\n",
    "    end_time_translation = time.time()\n",
    "    print('Prompt finished in (seconds): ', round(end_time_translation - start_time_translation, 2))\n",
    "    metrics = evaluate_translation(source=source_text, reference=reference_text, hypothesis=hypothesis)\n",
    "    print('Metric Calculation in (seconds): ', round(time.time() - end_time_translation, 2))\n",
    "\n",
    "    tmp_result = pd.DataFrame([{\n",
    "                'model': model_name,\n",
    "                'complexity': complexity,\n",
    "                'prompt_type': prompt_type,\n",
    "                'prompt': prompt,\n",
    "                'source_text': source_text,\n",
    "                'hypothesis': hypothesis,\n",
    "                'reference_text': reference_text,\n",
    "                'metrics': metrics\n",
    "            }])\n",
    "\n",
    "    # MLflow-Logging\n",
    "    experiment_name = f'{model_name}_{complexity}'\n",
    "\n",
    "    log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, tmp_result)\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    results = pd.concat([\n",
    "        results,\n",
    "        tmp_result\n",
    "    ], ignore_index=True)\n",
    "    return results"
   ],
   "id": "12fb076f8d17ce6a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 4 Execute Pipeline"
   ],
   "id": "51c523705a1b59e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T11:07:57.761195Z",
     "start_time": "2025-01-21T10:46:18.724694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translation_results = run_pipeline(data)\n",
    "translation_results.to_csv('translation_results.csv', sep=';')\n",
    "print('Pipeline abgeschlossen. Ergebnisse gespeichert.')"
   ],
   "id": "d34d572a8f458626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-32e09b5f-da68-45ab-afa4-8258f176b3b2', 'object': 'text_completion', 'created': 1737456378, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': \"\\n\\nThis text is about Felix and his best friend Lina's investigation into their mother's mysterious job.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 121, 'completion_tokens': 22, 'total_tokens': 143}}\n",
      "Prompt finished in (seconds):  83.69\n",
      "Metric Calculation in (seconds):  56.27\n",
      "üèÉ View run gemma/easy/zero_shot_to-english_englisch_1 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/c6a16fd5b2d34d8f9d1b7df24bfb88d3\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "{'id': 'cmpl-6801b34a-9c41-45f5-bec2-77ca00f31b62', 'object': 'text_completion', 'created': 1737456519, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': \"\\n\\n**English Translation:**\\n\\nFelix had sat: Mama was constantly on the go. But why this is the case, no one will tell him. Therefore, clear to Felix: his mother is a covert agent. When he receives a cryptic letter on his eleventh birthday, it seems his suspicion is confirmed. Together with his best friend Lina, he starts unraveling the mystery of his mother's job. When they finally manage to solve the case, they stick to their first exciting clue like detectives.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 122, 'completion_tokens': 100, 'total_tokens': 222}}\n",
      "Prompt finished in (seconds):  10.3\n",
      "Metric Calculation in (seconds):  37.4\n",
      "üèÉ View run gemma/easy/zero_shot-to-english_german_1 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/ec3697eb09c64abfb6cb50c690cbaa83\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "{'id': 'cmpl-c5b7dd33-63f3-46d9-8c8e-40699d58efec', 'object': 'text_completion', 'created': 1737456567, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 107, 'completion_tokens': 0, 'total_tokens': 107}}\n",
      "Prompt finished in (seconds):  3.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4052) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  24.95\n",
      "üèÉ View run gemma/easy/zero_shot_to-german_english_1 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/b8cce4f51f4c45369e88e247d41daec2\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "{'id': 'cmpl-18674b7c-137d-4df8-9cd0-981ab8dd6fb7', 'object': 'text_completion', 'created': 1737456596, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 108, 'completion_tokens': 0, 'total_tokens': 108}}\n",
      "Prompt finished in (seconds):  3.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4063) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  19.82\n",
      "üèÉ View run gemma/easy/zero_shot_to-german_german_1 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/972bb9e6bde54f73b6b4019e4f89879a\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n",
      "{'id': 'cmpl-ad5e79e5-e405-4d2d-8242-ae844013b5be', 'object': 'text_completion', 'created': 1737456619, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\nHere is the German text:\\n\\n\"Die rund 1.400 eingesetzten Beamten haben demnach beim Start in den Karneval sechs mutma√üliche Taschendiebe festgenommen und ermitteln nun zudem wegen mehreren F√§llen von K√∂rperverletzungen und Sexualdelikten.\"\\n\\nMy translation attempt:\\n\\n\"The approximately 1,400 deployed officers have thus apprehended six petty crime suspects at the start of the carnival and are now diligently investigating several cases of physical abuse and sexual assault. Precise crime statistics for the opening session of the carnival will be disclosed in the following week.\"\\n\\nPlease note that this is a translation attempt and may not be perfect. It is recommended to consult a professional translator for more accurate translations.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 77, 'completion_tokens': 145, 'total_tokens': 222}}\n",
      "Prompt finished in (seconds):  9.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4072) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  30.57\n",
      "üèÉ View run gemma/news_gen/zero_shot_to-english_englisch_1 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/f5b9a99758974ef9b0ac596fd5f83214\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "{'id': 'cmpl-2575c661-49f8-48e6-abfe-3bfd73ebe689', 'object': 'text_completion', 'created': 1737456659, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\nDer Text ist in Deutsch und m√∂chte die Situation auf dem Karneval beschreiben.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 17, 'total_tokens': 95}}\n",
      "Prompt finished in (seconds):  74.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4139) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  32.51\n",
      "üèÉ View run gemma/news_gen/zero_shot-to-english_german_1 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/6f888908498742dc9ab859506a5c4c71\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "{'id': 'cmpl-69772af8-2507-4f0a-840e-d2c7e1968c36', 'object': 'text_completion', 'created': 1737456766, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\nHere is the German translation I have come up with:\\n\\n\"Die circa 1.400 ausgef√ºhlene Streitkr√§fte haben sich beim Beginn des Carnesvals vernehmen und sind derzeit bereits auf mehrere F√§lle von K√∂rperverletzung und sexuellen Missbrauch hin untersucht. Die genaue Strafzahlen f√ºr den ersten Tag der Festlegung werden n√§chste Woche verf√ºgbar.\"', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 73, 'total_tokens': 139}}\n",
      "Prompt finished in (seconds):  11.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4174) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  29.63\n",
      "üèÉ View run gemma/news_gen/zero_shot_to-german_english_1 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/70027cc7dca54d8caaf29a2436e9c9db\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "{'id': 'cmpl-18700d23-54c0-43fb-8803-0428f74d7aad', 'object': 'text_completion', 'created': 1737456808, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\n**√úbersetzung:**\\n\\nDieapproximately 1.400 versetzten Mitarbeiter haben sich bereits 6 Vorfallverd√§chtigt die Startzeit des Karnevals angeglichen und sind derzeit auch die Untersuchung mehrerer Vorf√§lle von K√∂rperverletzung und sexuellen Vorf√§llen untersucht. Die genaue Strafzahlen f√ºr den ersten Tag des Karnevals werden n√§chsten Woche ver√∂ffentlicht.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 67, 'completion_tokens': 76, 'total_tokens': 143}}\n",
      "Prompt finished in (seconds):  9.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4197) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  27.5\n",
      "üèÉ View run gemma/news_gen/zero_shot_to-german_german_1 at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/e8911aa4d580494399790b052af76dd3\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n",
      "{'id': 'cmpl-705a627e-4366-42d8-9e4a-269930a72bb4', 'object': 'text_completion', 'created': 1737456845, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': \"\\n\\nHere is the German text translated into English:\\n\\n> The state chief also has the right to appoint interim ministers for a maximum of ten days during the Senate's recess periods. This ensures the operational capacity of the government. Ministers appointed in this manner must be confirmed by the Senate before the end of the recess period to continue in office.\\n\\n> The Republicans secured a majority in the Senate with at least 53 of the 100 seats. The Democrats could, however, delay the appointment process in the relevant committees.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 124, 'completion_tokens': 108, 'total_tokens': 232}}\n",
      "Prompt finished in (seconds):  7.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4212) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  25.34\n",
      "üèÉ View run gemma/news_spec/zero_shot_to-english_englisch_1 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/391bf39eee6c4e53a717daa460b76fc0\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "{'id': 'cmpl-4d694bd8-f2cf-4597-b353-dfc57647e1d1', 'object': 'text_completion', 'created': 1737456878, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': \"\\n\\nDie Regierung ist nicht bereit, den Staatschef zu verbieten, den Einsatz von Vorl√§ufenden Minister zu bewirken.\\n\\n**English Translation:**\\n\\nThe state chief also has the right to temporarily appoint a deputy minister during at least ten days of the Senate's recess period. This ensures the government's capacity to function effectively. Ministers appointed in this manner must be confirmed by the Senate before the end of the session to remain in office.\\n\\nThe Republican party secured a majority in the Senate with at least 53 out of 100 seats. The Democrats could, however, delay the appointment process in the relevant committees.\\n\\nThe government is not prepared to deny the state chief the right to use temporary ministers.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 125, 'completion_tokens': 148, 'total_tokens': 273}}\n",
      "Prompt finished in (seconds):  9.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4224) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  24.95\n",
      "üèÉ View run gemma/news_spec/zero_shot-to-english_german_1 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/bc8e6ec56fe84f52990a3782947ddd60\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "{'id': 'cmpl-bd32ef48-3516-45d0-9873-c5b3b13a334c', 'object': 'text_completion', 'created': 1737456913, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 109, 'completion_tokens': 0, 'total_tokens': 109}}\n",
      "Prompt finished in (seconds):  3.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4234) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  23.19\n",
      "üèÉ View run gemma/news_spec/zero_shot_to-german_english_1 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/5dbc8cbcdef04736b9eaf131884e00b0\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "{'id': 'cmpl-cd31271a-d765-4039-80c8-c6dfdd26f649', 'object': 'text_completion', 'created': 1737456940, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\nThe Senate also has the right to set a date for a vote of confidence in the head of state. This means that the head of state can be removed from office if a two-thirds majority of the Senate votes in favor.\\n\\nThe Republicans and Democrats are now locked in a heated debate about the power of the head of state. The Republicans argue that the head of state has the absolute power to appoint and dismiss ministers, while the Democrats argue that the head of state should have some limitations on this power.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 110, 'completion_tokens': 103, 'total_tokens': 213}}\n",
      "Prompt finished in (seconds):  7.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4249) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  29.24\n",
      "üèÉ View run gemma/news_spec/zero_shot_to-german_german_1 at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/2194b88e09e047c39096ab23706eae3a\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n",
      "{'id': 'cmpl-964df2c6-7db3-461e-9916-f8eeb18e907d', 'object': 'text_completion', 'created': 1737456977, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\n**Translation:**\\n\\nThe text is already in English. It is a description of research on the effects of climate change on heat waves in South Asia, and it concludes that heat waves have become more common and severe in the region.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 127, 'completion_tokens': 47, 'total_tokens': 174}}\n",
      "Prompt finished in (seconds):  157.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4318) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  28.25\n",
      "üèÉ View run gemma/pop_science/zero_shot_to-english_englisch_1 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/f10286733b0142ada4ea1bc352d1a525\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n",
      "{'id': 'cmpl-670a3122-97eb-4bb6-a858-fc8670c18b10', 'object': 'text_completion', 'created': 1737457163, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\n**English translation:**\\n\\nThe evidence supporting the hypothesis that climate change is intensifying heat waves in South Asia is overwhelming. Numerous studies attribute the increasing heat waves in South Asia to weather patterns and climate change. A report by World Weather Attribution concluded that the probability of heat waves has increased by 30-fold since the 19th century. Another study by the British Met Office showed that the risk of unprecedented heat waves in India and Pakistan has increased by more than 100-fold.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 128, 'completion_tokens': 102, 'total_tokens': 230}}\n",
      "Prompt finished in (seconds):  7.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4340) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  22.04\n",
      "üèÉ View run gemma/pop_science/zero_shot-to-english_german_1 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/1f1ac3d4668b404d855bb3dd9f5bfdd1\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n",
      "{'id': 'cmpl-1cabefd6-0e74-4ede-9c02-fb4a3d0a8eea', 'object': 'text_completion', 'created': 1737457193, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\nHere is the translated text:\\n\\n\"Es gibt keinen Frage, dass das Klimawandel die W√§rmehitze in S√ºd-Asien intensiviert. Kontinuierliche Attributionstudien, die die Beziehungen zwischen Wetterph√§nomenen und dem Klimawandel untersucht, best√§tigen dies. Zum Beispiel zeigt ein Bericht von World Weather Attribution, dass die Wahrscheinlichkeit von W√§rmezeiten sich seit dem 19. Jahrhundert 30-fach erh√∂ht hat. Dar√ºber hinaus eine Untersuchung des UK-Met-Offices zeigt, dass der Risiko f√ºr beispiellose W√§rmezeiten in Indien und Pakistan durch das Klimawandel 100-fach erh√∂ht wurde.\"', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 113, 'completion_tokens': 126, 'total_tokens': 239}}\n",
      "Prompt finished in (seconds):  10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4429) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  30.9\n",
      "üèÉ View run gemma/pop_science/zero_shot_to-german_english_1 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/8407848e9eaf423b99476865d4526d04\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n",
      "{'id': 'cmpl-02d74dcd-577c-46bd-8c71-c8a88b1bd0da', 'object': 'text_completion', 'created': 1737457234, 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/./gemma-2b-it-q8_0.gguf', 'choices': [{'text': '\\n\\nSo, what is the question?\\n\\nIch verstehe die Frage nicht.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 114, 'completion_tokens': 16, 'total_tokens': 130}}\n",
      "Prompt finished in (seconds):  70.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4577) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  33.13\n",
      "üèÉ View run gemma/pop_science/zero_shot_to-german_german_1 at: http://127.0.0.1:5000/#/experiments/819534162661537410/runs/3e8516b98cb74ddea8db62d46a538d35\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/819534162661537410\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m translation_results \u001B[38;5;241m=\u001B[39m run_pipeline(data)\n\u001B[1;32m      2\u001B[0m translation_results\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslation_results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPipeline abgeschlossen. Ergebnisse gespeichert.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[27], line 13\u001B[0m, in \u001B[0;36mrun_pipeline\u001B[0;34m(texts)\u001B[0m\n\u001B[1;32m     11\u001B[0m     complexity \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomplexity\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mnotna(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_german\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[0;32m---> 13\u001B[0m         results \u001B[38;5;241m=\u001B[39m execute_mlflow_run(complexity, model, model_name, prompt_type, results,\n\u001B[1;32m     14\u001B[0m                                      row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_german\u001B[39m\u001B[38;5;124m'\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_english\u001B[39m\u001B[38;5;124m'\u001B[39m], template)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# √úbersetzung Englisch -> Deutsch\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt_type, template \u001B[38;5;129;01min\u001B[39;00m PROMPT_TEMPLATES_ENGLISH_GERMAN\u001B[38;5;241m.\u001B[39mitems():\n",
      "Cell \u001B[0;32mIn[27], line 31\u001B[0m, in \u001B[0;36mexecute_mlflow_run\u001B[0;34m(complexity, model, model_name, prompt_type, results, source_text, reference_text, template)\u001B[0m\n\u001B[1;32m     28\u001B[0m prompt \u001B[38;5;241m=\u001B[39m template\u001B[38;5;241m.\u001B[39mformat(text\u001B[38;5;241m=\u001B[39msource_text)\n\u001B[1;32m     30\u001B[0m start_time_translation \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 31\u001B[0m hypothesis \u001B[38;5;241m=\u001B[39m translate(model, prompt, reference_text)\n\u001B[1;32m     32\u001B[0m end_time_translation \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrompt finished in (seconds): \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mround\u001B[39m(end_time_translation \u001B[38;5;241m-\u001B[39m start_time_translation, \u001B[38;5;241m2\u001B[39m))\n",
      "Cell \u001B[0;32mIn[19], line 8\u001B[0m, in \u001B[0;36mtranslate\u001B[0;34m(model, prompt, reference_translation)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# the model should not need more tokens than this\u001B[39;00m\n\u001B[1;32m      6\u001B[0m estimated_max_tokens \u001B[38;5;241m=\u001B[39m (token_length_prompt \u001B[38;5;241m+\u001B[39m token_length_ref) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1.5\u001B[39m\n\u001B[0;32m----> 8\u001B[0m response \u001B[38;5;241m=\u001B[39m model(prompt, max_tokens\u001B[38;5;241m=\u001B[39mestimated_max_tokens, echo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:1899\u001B[0m, in \u001B[0;36mLlama.__call__\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1835\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m   1836\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1837\u001B[0m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     logit_bias: Optional[Dict[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1862\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001B[1;32m   1863\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate text from a prompt.\u001B[39;00m\n\u001B[1;32m   1864\u001B[0m \n\u001B[1;32m   1865\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1897\u001B[0m \u001B[38;5;124;03m        Response object containing the generated text.\u001B[39;00m\n\u001B[1;32m   1898\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_completion(\n\u001B[1;32m   1900\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m   1901\u001B[0m         suffix\u001B[38;5;241m=\u001B[39msuffix,\n\u001B[1;32m   1902\u001B[0m         max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m   1903\u001B[0m         temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m   1904\u001B[0m         top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m   1905\u001B[0m         min_p\u001B[38;5;241m=\u001B[39mmin_p,\n\u001B[1;32m   1906\u001B[0m         typical_p\u001B[38;5;241m=\u001B[39mtypical_p,\n\u001B[1;32m   1907\u001B[0m         logprobs\u001B[38;5;241m=\u001B[39mlogprobs,\n\u001B[1;32m   1908\u001B[0m         echo\u001B[38;5;241m=\u001B[39mecho,\n\u001B[1;32m   1909\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m   1910\u001B[0m         frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m   1911\u001B[0m         presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m   1912\u001B[0m         repeat_penalty\u001B[38;5;241m=\u001B[39mrepeat_penalty,\n\u001B[1;32m   1913\u001B[0m         top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m   1914\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1915\u001B[0m         seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m   1916\u001B[0m         tfs_z\u001B[38;5;241m=\u001B[39mtfs_z,\n\u001B[1;32m   1917\u001B[0m         mirostat_mode\u001B[38;5;241m=\u001B[39mmirostat_mode,\n\u001B[1;32m   1918\u001B[0m         mirostat_tau\u001B[38;5;241m=\u001B[39mmirostat_tau,\n\u001B[1;32m   1919\u001B[0m         mirostat_eta\u001B[38;5;241m=\u001B[39mmirostat_eta,\n\u001B[1;32m   1920\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m   1921\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[1;32m   1922\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[1;32m   1923\u001B[0m         grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[1;32m   1924\u001B[0m         logit_bias\u001B[38;5;241m=\u001B[39mlogit_bias,\n\u001B[1;32m   1925\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001B[0m, in \u001B[0;36mLlama.create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1830\u001B[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001B[38;5;241m=\u001B[39m completion_or_chunks\n\u001B[1;32m   1831\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m chunks\n\u001B[0;32m-> 1832\u001B[0m completion: Completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(completion_or_chunks)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   1833\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m completion\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001B[0m, in \u001B[0;36mLlama._create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1315\u001B[0m finish_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1316\u001B[0m multibyte_fix \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1317\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m   1318\u001B[0m     prompt_tokens,\n\u001B[1;32m   1319\u001B[0m     top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m   1320\u001B[0m     top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m   1321\u001B[0m     min_p\u001B[38;5;241m=\u001B[39mmin_p,\n\u001B[1;32m   1322\u001B[0m     typical_p\u001B[38;5;241m=\u001B[39mtypical_p,\n\u001B[1;32m   1323\u001B[0m     temp\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m   1324\u001B[0m     tfs_z\u001B[38;5;241m=\u001B[39mtfs_z,\n\u001B[1;32m   1325\u001B[0m     mirostat_mode\u001B[38;5;241m=\u001B[39mmirostat_mode,\n\u001B[1;32m   1326\u001B[0m     mirostat_tau\u001B[38;5;241m=\u001B[39mmirostat_tau,\n\u001B[1;32m   1327\u001B[0m     mirostat_eta\u001B[38;5;241m=\u001B[39mmirostat_eta,\n\u001B[1;32m   1328\u001B[0m     frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m   1329\u001B[0m     presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m   1330\u001B[0m     repeat_penalty\u001B[38;5;241m=\u001B[39mrepeat_penalty,\n\u001B[1;32m   1331\u001B[0m     stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[1;32m   1332\u001B[0m     logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[1;32m   1333\u001B[0m     grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[1;32m   1334\u001B[0m ):\n\u001B[1;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m llama_cpp\u001B[38;5;241m.\u001B[39mllama_token_is_eog(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mmodel, token):\n\u001B[1;32m   1336\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetokenize(completion_tokens, prev_tokens\u001B[38;5;241m=\u001B[39mprompt_tokens)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001B[0m, in \u001B[0;36mLlama.generate\u001B[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001B[0m\n\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# Eval and sample\u001B[39;00m\n\u001B[1;32m    908\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 909\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval(tokens)\n\u001B[1;32m    910\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m sample_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tokens:\n\u001B[1;32m    911\u001B[0m         token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[1;32m    912\u001B[0m             top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m    913\u001B[0m             top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    927\u001B[0m             idx\u001B[38;5;241m=\u001B[39msample_idx,\n\u001B[1;32m    928\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001B[0m, in \u001B[0;36mLlama.eval\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    639\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch\u001B[38;5;241m.\u001B[39mset_batch(\n\u001B[1;32m    641\u001B[0m     batch\u001B[38;5;241m=\u001B[39mbatch, n_past\u001B[38;5;241m=\u001B[39mn_past, logits_all\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_params\u001B[38;5;241m.\u001B[39mlogits_all\n\u001B[1;32m    642\u001B[0m )\n\u001B[0;32m--> 643\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ctx\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch)\n\u001B[1;32m    644\u001B[0m \u001B[38;5;66;03m# Save tokens\u001B[39;00m\n\u001B[1;32m    645\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_ids[n_past : n_past \u001B[38;5;241m+\u001B[39m n_tokens] \u001B[38;5;241m=\u001B[39m batch\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001B[0m, in \u001B[0;36mLlamaContext.decode\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: LlamaBatch):\n\u001B[0;32m--> 300\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m llama_cpp\u001B[38;5;241m.\u001B[39mllama_decode(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx,\n\u001B[1;32m    302\u001B[0m         batch\u001B[38;5;241m.\u001B[39mbatch,\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_decode returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
