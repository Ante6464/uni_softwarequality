{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jupyter Notebook for Project \"Comparison of LLM Prompting Techniques\"",
   "id": "962e2691489cf6fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import sacrebleu\n",
    "from sacrebleu import corpus_bleu\n",
    "import hashlib\n",
    "from llama_cpp import Llama\n",
    "from sympy import false"
   ],
   "id": "e2398ca7b5ff8ed5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading",
   "id": "fee57aff9aaf29b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"machine_translation.pkl\")\n",
    "data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['complexity'] = data['complexity']\n",
    "data_info['text_german_length'] = data['text_german'].str.len()\n",
    "data_info['text_english_length'] = data['text_english'].str.len()\n",
    "data_info"
   ],
   "id": "7ff3eb7d21b8b721"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Loading",
   "id": "bd9f6432346f6748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modelle laden\n",
    "gemma = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-ai/gemma-2b-it-GGUF\",\n",
    "\tfilename=\"gemma-2b-it-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama32 = Llama.from_pretrained(\n",
    "\trepo_id=\"hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\",\n",
    "\tfilename=\"llama-3.2-3b-instruct-q8_0.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama31 = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "\tfilename=\"Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "aya23 = Llama.from_pretrained(\n",
    "\trepo_id=\"bartowski/aya-23-35B-GGUF\",\n",
    "\tfilename=\"aya-23-35B-Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    \"gemma\": gemma,\n",
    "    #\"llama32\": llama32,\n",
    "    #\"llama31\": llama31,\n",
    "    #\"aya23\": aya23,\n",
    "}"
   ],
   "id": "27d6d6528b3dcb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metric Setup",
   "id": "406e69e94455faa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[GitHub Repo to MatricX](https://github.com/google-research/metricx)",
   "id": "d75e732dc202507a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    \"\"\"\n",
    "\n",
    "    data = [{\"id\": \"1\", \"source\": source, \"reference\": reference, \"hypothesis\": hypothesis}]\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = \"./temp_input.jsonl\"\n",
    "    output_file = \"./temp_output.jsonl\"\n",
    "    model = \"google/metricx-24-hybrid-large-v2p6-bfloat16\"\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        command = [\n",
    "            \"python\", \"-m\", \"metricx24.predict\",\n",
    "            \"--tokenizer\", \"google/mt5-xl\",\n",
    "            \"--model_name_or_path\", model,\n",
    "            \"--max_input_length\", \"1536\",\n",
    "            \"--batch_size\", \"1\",\n",
    "            \"--input_file\", input_file,\n",
    "            \"--output_file\", output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        # for line in process.stdout:\n",
    "        #     print(line, end=\"\")\n",
    "        # for line in process.stderr:\n",
    "        #     print(f\"ERROR: {line}\", end=\"\")\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "             print(f\"Error executing metricx24. Return code: {process.returncode}\")\n",
    "             return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get(\"prediction\"))  # Ensure that \"score\" exists\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print(\"Error parsing the output file.\")\n",
    "                    return None\n",
    "\n",
    "        return None # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass #If the files don't exist for some reason, the error is caught\n"
   ],
   "id": "4a5ab47796005682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example call\n",
    "source_text = \"I am learning Python for Machine Learning.\"\n",
    "reference_text = \"I am learning Python for machine learning.\"\n",
    "hypothesis_text = \"I'm studying Python for machine learning.\"\n",
    "\n",
    "score = calculate_metricx_score(source_text, reference_text, hypothesis_text)\n",
    "\n",
    "if score is not None:\n",
    "    print(f\"The calculated score is: {score}\")\n",
    "else:\n",
    "    print(\"The score calculation failed.\")"
   ],
   "id": "f4c6295c686cc946"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## zweite Möglichkeit metricx zu nutzen...\n",
    "import sys\n",
    "from metricx24 import predict\n",
    "\n",
    "# Simuliere Kommandozeilenargumente\n",
    "sys.argv = [\n",
    "    \"predict.py\",  # Name des Skripts (wird ignoriert)\n",
    "    \"--tokenizer\", \"google/mt5-xl\",\n",
    "    \"--model_name_or_path\", \"google/metricx-24-hybrid-large-v2p6-bfloat16\",\n",
    "    \"--max_input_length\", \"1536\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--input_file\", \"./input.jsonl\",\n",
    "    \"--output_file\", \"./output.jsonl\"\n",
    "]\n",
    "\n",
    "# Rufe die main-Funktion auf\n",
    "predict.main()\n",
    "\n",
    "print(\"Vorhersage abgeschlossen. Ergebnisse sind in 'output.jsonl' gespeichert.\")"
   ],
   "id": "f492ee811ec3fd36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "62ac1a6e2dbdae96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pipeline",
   "id": "8e5396fec902551e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Composition",
   "id": "4dba8b573c0379b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Beide Richtungen abbilden: English <-> German\n",
    "# TODO: Verschiedene Prompt Arten: zero-shot, few-shot und verschiedene Variationen reinbringen\n",
    "\n",
    "PROMPT_TEMPLATES_ENGLISH_GERMAN = {\n",
    "    \"zero_shot-english\": \"Please translate the following text from English to German: {text}\",\n",
    "    #\"zero_shot-german\": f\"Bitte übersetze den folgenden Text von Englisch nach Deutsch: {text}\",\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from English to German.\n",
    "    #Here are some examples:\n",
    "    #- English: \"Hello\" -> German: \"Hallo\"\n",
    "    #- English: \"Goodbye\" -> German: \"Auf Wiedersehen\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte übersetze einen Text von Englisch nach Deutsch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Englisch: \"Hello\" -> Deutsch: \"Hallo\"\n",
    "    #- Englisch: \"Goodbye\" -> Deutsch: \"Auf Wiedersehen\"\n",
    "    #Jetzt übersetze diesen Text: {text}\"\"\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES_GERMAN_ENGLISH = {\n",
    "    \"zero_shot-english\": \"Please translate the following text from German to English: {text}\",\n",
    "    #\"zero_shot-german\": f\"Bitte übersetze den folgenden Text von Deutsch nach Englisch: {text}\",\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from German to English.\n",
    "    #Here are some examples:\n",
    "    #- German: \"Hallo\" -> English: \"Hello\"\n",
    "    #- German: \"Auf Wiedersehen\" -> English: \"Goodbye\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte übersetze einen Text von Deutsch nach Englisch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Deutsch: \"Hallo\" -> Englisch: \"Hello\"\n",
    "    #- Deutsch: \"Auf Wiedersehen\" -> Englisch: \"Goodbye\"\n",
    "    #Jetzt übersetze diesen Text: {text}\"\"\",\n",
    "}"
   ],
   "id": "e70812e535e26c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Interaction",
   "id": "cb00365dd1aad494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def translate(model, prompt):\n",
    "    response = model(prompt, max_tokens=512, echo=False, stop=[\"Q:\"])\n",
    "    return response['choices'][0]['text']"
   ],
   "id": "50fdfbda377216c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "23525c106ed337fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_translation(reference, translation):\n",
    "    bleu_score = corpus_bleu([translation], [[reference]]).score\n",
    "    return {\"BLEU\": bleu_score}"
   ],
   "id": "68b3d386c2d373aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    if experiment:\n",
    "        if experiment.lifecycle_stage == \"deleted\":\n",
    "            mlflow.tracking.MlflowClient().restore_experiment(experiment.experiment_id)\n",
    "            #mlflow.delete_experiment(experiment.experiment_id)\n",
    "    else:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f\"{model_name}/{complexity}/{prompt_type}\"):\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_param(\"complexity\", complexity)\n",
    "        mlflow.log_param(\"prompt_type\", prompt_type)\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)"
   ],
   "id": "770d3703528c2548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Pipeline Method",
   "id": "36fde1001499c4c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_pipeline(texts):\n",
    "    results = pd.DataFrame(columns=[\"model\", \"complexity\", \"prompt_type\", \"prompt\", \"source_text\", \"translation\", \"metrics\"])\n",
    "    mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "    for model_name, model in MODELS.items():\n",
    "        for _, row in texts.iterrows():\n",
    "\n",
    "            # Übersetzung Deutsch -> Englisch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_GERMAN_ENGLISH.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_german']):\n",
    "                    prompt = template.format(text=row['text_german'])\n",
    "                    translation = translate(model, prompt)\n",
    "                    metrics = evaluate_translation(reference=row['text_english'], translation=translation)\n",
    "\n",
    "                    # MLflow-Logging\n",
    "                    experiment_name = f\"{model_name}_{complexity}\"\n",
    "                    log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity)\n",
    "\n",
    "                    # Ergebnis speichern\n",
    "                    results = pd.concat([\n",
    "                        results,\n",
    "                        pd.DataFrame([{\n",
    "                            \"model\": model_name,\n",
    "                            \"complexity\": complexity,\n",
    "                            \"prompt_type\": prompt_type,\n",
    "                            \"prompt\": prompt,\n",
    "                            \"source_text\": row['text_german'],\n",
    "                            \"translation\": translation,\n",
    "                            \"metrics\": metrics\n",
    "                        }])\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "                    #results.to_csv(\"results.csv\", index=false)\n",
    "                    #mlflow.log_artifact(\"results.csv\")\n",
    "\n",
    "            # Übersetzung Englisch -> Deutsch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_ENGLISH_GERMAN.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_english']):\n",
    "                    prompt = template.format(text=row['text_english'])\n",
    "                    translation = translate(model, prompt)\n",
    "                    metrics = evaluate_translation(reference=row['text_german'], translation=translation)\n",
    "\n",
    "                    # MLflow-Logging\n",
    "                    experiment_name = f\"{model_name}_{complexity}\"\n",
    "                    log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity)\n",
    "\n",
    "                    # Ergebnis speichern\n",
    "                    results = pd.concat([\n",
    "                        results,\n",
    "                        pd.DataFrame([{\n",
    "                            \"model\": model_name,\n",
    "                            \"complexity\": complexity,\n",
    "                            \"prompt_type\": prompt_type,\n",
    "                            \"prompt\": prompt,\n",
    "                            \"source_text\": row['text_english'],\n",
    "                            \"translation\": translation,\n",
    "                            \"metrics\": metrics\n",
    "                        }])\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "                    #results.to_csv(\"results.csv\", index=false)\n",
    "                    #mlflow.log_artifact(\"results.csv\")\n",
    "\n",
    "    return results"
   ],
   "id": "12fb076f8d17ce6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Execute Pipeline",
   "id": "51c523705a1b59e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "translation_results = run_pipeline(data)\n",
    "translation_results.to_csv(\"translation_results.csv\", index=false)\n",
    "print(\"Pipeline abgeschlossen. Ergebnisse gespeichert.\")"
   ],
   "id": "d34d572a8f458626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "translation_results",
   "id": "4721ba5c1793230e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mlflow.end_run()",
   "id": "b911e753a5baa759"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
