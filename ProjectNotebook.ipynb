{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jupyter Notebook for Project \"Comparison of LLM Prompting Techniques\"",
   "id": "962e2691489cf6fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:29.054474Z",
     "start_time": "2025-01-21T12:17:29.049708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import sacrebleu\n",
    "from llama_cpp import Llama\n",
    "import time\n"
   ],
   "id": "e2398ca7b5ff8ed5",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 Data Loading\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe."
   ],
   "id": "fee57aff9aaf29b2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:29.150487Z",
     "start_time": "2025-01-21T12:17:29.139310Z"
    }
   },
   "source": [
    "data = pd.read_pickle('machine_translation.pkl')\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity                                        text_german  \\\n",
       "0         easy  Felix hat es satt: St√§ndig ist Mama unterwegs....   \n",
       "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
       "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
       "3  pop_science  Dass der Klimawandel die Hitzewellen in S√ºdasi...   \n",
       "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
       "\n",
       "                                        text_english  \n",
       "0  Felix is fed up: Mom is always on the go. But ...  \n",
       "1  The approximately 1,400 deployed officers have...  \n",
       "2  The head of state also has the right to appoin...  \n",
       "3  There is no question that climate change is in...  \n",
       "4  The DSA-110, situated at the Owens Valley Radi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german</th>\n",
       "      <th>text_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>Felix hat es satt: St√§ndig ist Mama unterwegs....</td>\n",
       "      <td>Felix is fed up: Mom is always on the go. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>Die rund 1.400 eingesetzten Beamten haben demn...</td>\n",
       "      <td>The approximately 1,400 deployed officers have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>Der Staatschef hat zugleich aber das Recht, vo...</td>\n",
       "      <td>The head of state also has the right to appoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>Dass der Klimawandel die Hitzewellen in S√ºdasi...</td>\n",
       "      <td>There is no question that climate change is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Der DSA-110, der sich am Owens Valley Radio Ob...</td>\n",
       "      <td>The DSA-110, situated at the Owens Valley Radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:29.191518Z",
     "start_time": "2025-01-21T12:17:29.180734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['complexity'] = data['complexity']\n",
    "data_info['text_german_length'] = data['text_german'].str.len()\n",
    "data_info['text_english_length'] = data['text_english'].str.len()\n",
    "data_info"
   ],
   "id": "7ff3eb7d21b8b721",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity  text_german_length  text_english_length\n",
       "0         easy                 485                  415\n",
       "1     news_gen                 296                  280\n",
       "2    news_spec                 518                  484\n",
       "3  pop_science                 542                  521\n",
       "4      science                1003                  827"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german_length</th>\n",
       "      <th>text_english_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>485</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>296</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>518</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>542</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>1003</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 2 Model Loading\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp:\n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python"
   ],
   "id": "bd9f6432346f6748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.067205Z",
     "start_time": "2025-01-21T12:17:29.250331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modelle laden\n",
    "gemma = Llama.from_pretrained(\n",
    "    repo_id='lmstudio-ai/gemma-2b-it-GGUF',\n",
    "    filename='gemma-2b-it-q8_0.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama32 = Llama.from_pretrained(\n",
    "    repo_id='hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF',\n",
    "    filename='llama-3.2-3b-instruct-q8_0.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama31 = Llama.from_pretrained(\n",
    "    repo_id='lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "    filename='Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "aya23 = Llama.from_pretrained(\n",
    "    repo_id='bartowski/aya-23-35B-GGUF',\n",
    "    filename='aya-23-35B-Q5_K_M.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    'gemma': gemma,\n",
    "    #'llama32': llama32,\n",
    "    #'llama31': llama31,\n",
    "    #'aya23': aya23,\n",
    "}"
   ],
   "id": "27d6d6528b3dcb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "62ac1a6e2dbdae96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Pipeline",
   "id": "8e5396fec902551e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Prompt Composition",
   "id": "4dba8b573c0379b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.085046Z",
     "start_time": "2025-01-21T12:17:32.083109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Beide Richtungen abbilden: English <-> German\n",
    "# TODO: Verschiedene Prompt Arten: zero-shot, few-shot und verschiedene Variationen reinbringen\n",
    "\n",
    "PROMPT_TEMPLATES_ENGLISH_GERMAN = {\n",
    "    'zero_shot_to-german_english_1': 'Please translate the following text from English to German: \\\"{text}\\\"',\n",
    "    # 'zero_shot_to-german_german_1': 'Bitte √ºbersetze den folgenden Text von Englisch nach Deutsch: {text}',\n",
    "\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from English to German.\n",
    "    #Here are some examples:\n",
    "    #- English: \"Hello\" -> German: \"Hallo\"\n",
    "    #- English: \"Goodbye\" -> German: \"Auf Wiedersehen\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte √ºbersetze einen Text von Englisch nach Deutsch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Englisch: \"Hello\" -> Deutsch: \"Hallo\"\n",
    "    #- Englisch: \"Goodbye\" -> Deutsch: \"Auf Wiedersehen\"\n",
    "    #Jetzt √ºbersetze diesen Text: {text}\"\"\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES_GERMAN_ENGLISH = {\n",
    "    'zero_shot_to-english_englisch_1': 'Please translate the following text from German to English: \\\"{text}\\\"',\n",
    "    # 'zero_shot-to-english_german_1': 'Bitte √ºbersetze den folgenden Text von Deutsch nach Englisch: {text}',\n",
    "\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from German to English.\n",
    "    #Here are some examples:\n",
    "    #- German: \"Hallo\" -> English: \"Hello\"\n",
    "    #- German: \"Auf Wiedersehen\" -> English: \"Goodbye\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte √ºbersetze einen Text von Deutsch nach Englisch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Deutsch: \"Hallo\" -> Englisch: \"Hello\"\n",
    "    #- Deutsch: \"Auf Wiedersehen\" -> Englisch: \"Goodbye\"\n",
    "    #Jetzt √ºbersetze diesen Text: {text}\"\"\",\n",
    "}"
   ],
   "id": "e70812e535e26c4",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Model Interaction",
   "id": "cb00365dd1aad494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.101526Z",
     "start_time": "2025-01-21T12:17:32.099589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(model, prompt, reference_translation):\n",
    "    # we estimate the needed max_tokens based on the tokenized prompt and reference_translation\n",
    "    token_length_ref = len(model.tokenize(reference_translation.encode('utf-8')))\n",
    "    token_length_prompt = len(model.tokenize(prompt.encode('utf-8')))\n",
    "    # the model should not need more tokens than this\n",
    "    estimated_max_tokens = (token_length_prompt + token_length_ref) * 1.5\n",
    "\n",
    "    response = model(prompt, max_tokens=estimated_max_tokens, echo=False)\n",
    "    print(response['choices'][0]['text'])\n",
    "    return response['choices'][0]['text']"
   ],
   "id": "50fdfbda377216c8",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.164963Z",
     "start_time": "2025-01-21T12:17:32.163290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt3 = 'Please translate the following text into German. Begin the translation with <<< and end it with >>>: The approximately 1,400 deployed officers have therefore arrested six suspected pickpockets at the start of the carnival and are now also investigating several cases of bodily harm and sexual offenses. Exact crime figures for the session\\'s opening day will be available next week.'\n",
    "\n",
    "# response = gemma.create_completion(prompt3, echo=False, max_tokens=200)\n",
    "\n",
    "# print(response)\n"
   ],
   "id": "49d48844fe5c7b16",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.173743Z",
     "start_time": "2025-01-21T12:17:32.169673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = 'Der Staatschef hat zugleich aber das Recht, vorl√§ufig Minister w√§hrend mindestens zehn Tage langen Sitzungspausen des Senats einzusetzen. Das soll die Handlungsf√§higkeit der Regierung gew√§hrleisten. Die so ernannten Minister m√ºssen dann bis Ende der Sitzungsperiode vom Senat best√§tigt werden, um weiter im Amt zu bleiben.\\n\\nDie Republikaner sicherten sich bei der Wahl eine Mehrheit im Senat mit mindestens 53 der 100 Sitze. Die Demokraten k√∂nnten aber das Ernennungsverfahren in den zust√§ndigen Aussch√ºssen verz√∂gern.'\n",
    "\n",
    "prompt2 = 'The head of state also has the right to appoint interim ministers during Senate recesses lasting at least ten days. This is to ensure the government\\'s ability to function. The ministers appointed in this manner must be confirmed by the Senate by the end of the session period to remain in office.\\n\\nThe Republicans secured a majority in the Senate with at least 53 of the 100 seats in the election. However, the Democrats could delay the appointment process in the relevant committees.'\n",
    "\n",
    "prompt3 = 'The approximately 1,400 deployed officers have therefore arrested six suspected pickpockets at the start of the carnival and are now also investigating several cases of bodily harm and sexual offenses. Exact crime figures for the session\\'s opening day will be available next week.'\n",
    "\n",
    "prompt4 = 'Die rund 1.400 eingesetzten Beamten haben demnach beim Start in den Karneval sechs mutma√üliche Taschendiebe festgenommen und ermitteln nun zudem wegen mehreren F√§llen von K√∂rperverletzungen und Sexualdelikten. Genaue Zahlen zur Kriminalit√§t am Sessionsauftakt soll es in der n√§chsten Woche geben.'\n",
    "\n",
    "print(len(llama31.tokenize(prompt2.encode('utf-8'))))\n",
    "print(len(llama31.tokenize(prompt.encode('utf-8'))))\n",
    "print(len(llama31.tokenize(prompt3.encode('utf-8'))))\n",
    "print(len(llama31.tokenize(prompt4.encode('utf-8'))))\n",
    "# response = llama31.create_completion(prompt, echo=False, max_tokens=400)\n",
    "# print(response)"
   ],
   "id": "f5193cb323a71aed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "145\n",
      "52\n",
      "86\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.242788Z",
     "start_time": "2025-01-21T12:17:32.239303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "{'id': 'cmpl-6d176ba7-9ab4-4fec-8522-25321557636c', 'object': 'text_completion', 'created': 1737399536,\n",
    " 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/./Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
    " 'choices': [{\n",
    "     'text': '\"\\n\\nHere is the translation of the text from German to English:\\n\\n\"The head of state has the right, however, to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions. This is to ensure the government\\'s operational ability. The ministers thus appointed must then be confirmed by the Senate by the end of the legislative period in order to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\"\\n\\nLet me know if you need any further assistance! \\n(Translation is provided in a neutral tone and does not imply any particular interpretation or opinion.) \\n\\nI have translated the text into English, maintaining a neutral tone and avoiding any interpretation or opinion. If you require any further assistance or have any specific requests, please feel free to ask! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. Ministers thus appointed must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\\n\\nPlease let me know if you have any further requests or need any additional assistance! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. The ministers appointed in this way must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured a majority in',\n",
    "     'index': 0, 'logprobs': None, 'finish_reason': 'length'}],\n",
    " 'usage': {'prompt_tokens': 156, 'completion_tokens': 356, 'total_tokens': 512}}"
   ],
   "id": "88899180bca30d13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-6d176ba7-9ab4-4fec-8522-25321557636c',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1737399536,\n",
       " 'model': '/Users/fynn/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/./Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
       " 'choices': [{'text': '\"\\n\\nHere is the translation of the text from German to English:\\n\\n\"The head of state has the right, however, to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions. This is to ensure the government\\'s operational ability. The ministers thus appointed must then be confirmed by the Senate by the end of the legislative period in order to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\"\\n\\nLet me know if you need any further assistance! \\n(Translation is provided in a neutral tone and does not imply any particular interpretation or opinion.) \\n\\nI have translated the text into English, maintaining a neutral tone and avoiding any interpretation or opinion. If you require any further assistance or have any specific requests, please feel free to ask! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. Ministers thus appointed must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured themselves a majority in the Senate with at least 53 of the 100 seats. However, the Democrats could delay the appointment procedure in the relevant committees.\"\\n\\nPlease let me know if you have any further requests or need any additional assistance! \\n\\nHere is the translation:\\n\\n\"The head of state has the right to appoint ministers temporarily during at least ten-day recesses of the Senate\\'s sessions, thereby ensuring the government\\'s operational ability. The ministers appointed in this way must then be confirmed by the Senate by the end of the legislative period to remain in office.\\n\\nThe Republicans secured a majority in',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 156,\n",
       "  'completion_tokens': 356,\n",
       "  'total_tokens': 512}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Metrics Calculation\n",
    "[GitHub Repo to MetricX](https://github.com/google-research/metricx)"
   ],
   "id": "23525c106ed337fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.313351Z",
     "start_time": "2025-01-21T12:17:32.310323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    '''\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    '''\n",
    "\n",
    "    data = [{'id': '1', 'source': source, 'reference': reference, 'hypothesis': hypothesis}]\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = './temp_input.jsonl'\n",
    "    output_file = './temp_output.jsonl'\n",
    "    model = 'google/metricx-24-hybrid-large-v2p6-bfloat16'\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'w', encoding='utf-8') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        command = [\n",
    "            'python', '-m', 'metricx24.predict',\n",
    "            '--tokenizer', 'google/mt5-xl',\n",
    "            '--model_name_or_path', model,\n",
    "            '--max_input_length', '1536',\n",
    "            '--batch_size', '1',\n",
    "            '--input_file', input_file,\n",
    "            '--output_file', output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        # for line in process.stdout:\n",
    "        #     print(line, end='')\n",
    "        # for line in process.stderr:\n",
    "        #     print(f'ERROR: {line}', end='')\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f'Error executing metricx24. Return code: {process.returncode}')\n",
    "            return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get('prediction'))  # Ensure that 'score' exists\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print('Error parsing the output file.')\n",
    "                    return None\n",
    "\n",
    "        return None  # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass  #If the files don't exist for some reason, the error is caught\n"
   ],
   "id": "4a5ab47796005682",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.327789Z",
     "start_time": "2025-01-21T12:17:32.326258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example call\n",
    "source_text = 'I am learning Python for Machine Learning.'\n",
    "reference_text = 'I am learning Python for machine learning.'\n",
    "hypothesis_text = \"I'm studying Python for machine learning.\"\n",
    "\n",
    "# score = calculate_metricx_score(source_text, reference_text, hypothesis_text)\n",
    "#\n",
    "# if score is not None:\n",
    "#     print(f'The calculated score is: {score}')\n",
    "# else:\n",
    "#     print('The score calculation failed.')"
   ],
   "id": "be6a0a27f7ff0d5",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.334660Z",
     "start_time": "2025-01-21T12:17:32.332860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_translation(source, reference, hypothesis):\n",
    "    #TODO: ich wei√ü nicht, ob wir die Methoden von sacrebleu richtig benutzen\n",
    "    #   oder ob wir die Strings vielleicht noch in einzelne S√§tze splitten m√ºssen\n",
    "    bleu_score = sacrebleu.corpus_bleu([hypothesis], [[reference]]).score\n",
    "    chrf_score = sacrebleu.corpus_chrf([hypothesis], [[reference]]).score\n",
    "    metricx_score = calculate_metricx_score(source, reference, hypothesis)\n",
    "    if metricx_score is None:\n",
    "        metricx_score = -1\n",
    "\n",
    "    return {'BLEU': bleu_score,\n",
    "            'chrF': chrf_score,\n",
    "            'MetricX': metricx_score}"
   ],
   "id": "68b3d386c2d373aa",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Logging to MLFLow",
   "id": "3743c34bde1b7d77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.340941Z",
     "start_time": "2025-01-21T12:17:32.338609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, target_language, tmp_result):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    if experiment:\n",
    "        if experiment.lifecycle_stage == 'deleted':\n",
    "            mlflow.tracking.MlflowClient().restore_experiment(experiment.experiment_id)\n",
    "            #mlflow.delete_experiment(experiment.experiment_id)\n",
    "    else:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f'{model_name}/{complexity}/{prompt_type}'):\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('complexity', complexity)\n",
    "        mlflow.log_param('prompt_type', prompt_type)\n",
    "        mlflow.log_param('target_language', target_language)\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        tmp_result.to_json('tmp_results.json', index=False)\n",
    "        mlflow.log_artifact('tmp_results.json')\n",
    "        mlflow.end_run()\n"
   ],
   "id": "770d3703528c2548",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 Pipeline Composition",
   "id": "36fde1001499c4c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:32.352247Z",
     "start_time": "2025-01-21T12:17:32.349107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline(texts):\n",
    "    results = pd.DataFrame(\n",
    "        columns=['model', 'complexity', 'prompt_type', 'prompt', 'source_text', 'hypothesis', 'reference', 'metrics'])\n",
    "    mlflow.set_tracking_uri(uri='http://127.0.0.1:5000')\n",
    "\n",
    "    for model_name, model in MODELS.items():\n",
    "        for _, row in texts.iterrows():\n",
    "\n",
    "            # √úbersetzung Deutsch -> Englisch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_GERMAN_ENGLISH.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_german']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, 'English', results,\n",
    "                                                 row['text_german'], row['text_english'], template)\n",
    "\n",
    "            # √úbersetzung Englisch -> Deutsch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_ENGLISH_GERMAN.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_english']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, 'German', results,\n",
    "                                                 row['text_english'], row['text_german'], template)\n",
    "\n",
    "    # results.to_csv('results.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "def execute_mlflow_run(complexity, model, model_name, prompt_type, target_language, results, source_text,\n",
    "                       reference_text, template):\n",
    "    prompt = template.format(text=source_text)\n",
    "\n",
    "    start_time_translation = time.time()\n",
    "    hypothesis = translate(model, prompt, reference_text)\n",
    "    end_time_translation = time.time()\n",
    "    print('Prompt finished in (seconds): ', round(end_time_translation - start_time_translation, 2))\n",
    "    metrics = evaluate_translation(source=source_text, reference=reference_text, hypothesis=hypothesis)\n",
    "    print('Metric Calculation in (seconds): ', round(time.time() - end_time_translation, 2))\n",
    "\n",
    "    tmp_result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'complexity': complexity,\n",
    "        'prompt_type': prompt_type,\n",
    "        'prompt': prompt,\n",
    "        'source_text': source_text,\n",
    "        'hypothesis': hypothesis,\n",
    "        'reference_text': reference_text,\n",
    "        'metrics': metrics\n",
    "    }])\n",
    "\n",
    "    # MLflow-Logging\n",
    "    experiment_name = f'{model_name}_{complexity}'\n",
    "\n",
    "    log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, target_language, tmp_result)\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    results = pd.concat([\n",
    "        results,\n",
    "        tmp_result\n",
    "    ], ignore_index=True)\n",
    "    return results"
   ],
   "id": "12fb076f8d17ce6a",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 4 Execute Pipeline"
   ],
   "id": "51c523705a1b59e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:20:14.995929Z",
     "start_time": "2025-01-21T12:17:32.362393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translation_results = run_pipeline(data)\n",
    "translation_results.to_csv('translation_results.csv', sep=';')\n",
    "print('Pipeline abgeschlossen. Ergebnisse gespeichert.')"
   ],
   "id": "d34d572a8f458626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This text is about Felix and his best friend Lina's investigation into their mother's mysterious job.\n",
      "Prompt finished in (seconds):  87.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(6043) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Calculation in (seconds):  40.09\n",
      "üèÉ View run gemma/easy/zero_shot_to-english_englisch_1 at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/902d5075ea0b4cfeaa4031092be30e33\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
