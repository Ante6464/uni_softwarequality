{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jupyter Notebook for Project \"Comparison of LLM Prompting Techniques\"",
   "id": "962e2691489cf6fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:42.364977Z",
     "start_time": "2025-01-20T09:50:42.359445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import sacrebleu\n",
    "from sacrebleu import corpus_bleu\n",
    "import hashlib\n",
    "from llama_cpp import Llama\n",
    "import time\n"
   ],
   "id": "e2398ca7b5ff8ed5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loading\n",
    "In the first step we import the given translations as pandas Dataframes and print a quick overview of the dataframe."
   ],
   "id": "fee57aff9aaf29b2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:42.443754Z",
     "start_time": "2025-01-20T09:50:42.437348Z"
    }
   },
   "source": [
    "data = pd.read_pickle('machine_translation.pkl')\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity                                        text_german  \\\n",
       "0         easy  Felix hat es satt: Ständig ist Mama unterwegs....   \n",
       "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
       "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
       "3  pop_science  Dass der Klimawandel die Hitzewellen in Südasi...   \n",
       "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
       "\n",
       "                                        text_english  \n",
       "0  Felix is fed up: Mom is always on the go. But ...  \n",
       "1  The approximately 1,400 deployed officers have...  \n",
       "2  The head of state also has the right to appoin...  \n",
       "3  There is no question that climate change is in...  \n",
       "4  The DSA-110, situated at the Owens Valley Radi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german</th>\n",
       "      <th>text_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>Felix hat es satt: Ständig ist Mama unterwegs....</td>\n",
       "      <td>Felix is fed up: Mom is always on the go. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>Die rund 1.400 eingesetzten Beamten haben demn...</td>\n",
       "      <td>The approximately 1,400 deployed officers have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>Der Staatschef hat zugleich aber das Recht, vo...</td>\n",
       "      <td>The head of state also has the right to appoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>Dass der Klimawandel die Hitzewellen in Südasi...</td>\n",
       "      <td>There is no question that climate change is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Der DSA-110, der sich am Owens Valley Radio Ob...</td>\n",
       "      <td>The DSA-110, situated at the Owens Valley Radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:42.496625Z",
     "start_time": "2025-01-20T09:50:42.488120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['complexity'] = data['complexity']\n",
    "data_info['text_german_length'] = data['text_german'].str.len()\n",
    "data_info['text_english_length'] = data['text_english'].str.len()\n",
    "data_info"
   ],
   "id": "7ff3eb7d21b8b721",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity  text_german_length  text_english_length\n",
       "0         easy                 485                  415\n",
       "1     news_gen                 296                  280\n",
       "2    news_spec                 518                  484\n",
       "3  pop_science                 542                  521\n",
       "4      science                1003                  827"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german_length</th>\n",
       "      <th>text_english_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>485</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>296</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>518</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>542</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>1003</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Loading\n",
    "In the second step we import the AI-Models which are given in the specified task. For doing so we use the `llama-cpp-python` library (further documentation can be found [here](https://github.com/abetlen/llama-cpp-python)) and import the models directly from [huggingface](https://huggingface.co/).\n",
    "\n",
    "Quick overview and installation guide of llama.cpp:\n",
    "- https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- https://christophergs.com/blog/running-open-source-llms-in-python"
   ],
   "id": "bd9f6432346f6748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.116619Z",
     "start_time": "2025-01-20T09:50:42.576012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modelle laden\n",
    "gemma = Llama.from_pretrained(\n",
    "    repo_id='lmstudio-ai/gemma-2b-it-GGUF',\n",
    "    filename='gemma-2b-it-q8_0.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama32 = Llama.from_pretrained(\n",
    "    repo_id='hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF',\n",
    "    filename='llama-3.2-3b-instruct-q8_0.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llama31 = Llama.from_pretrained(\n",
    "    repo_id='lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "    filename='Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf',\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "aya23 = Llama.from_pretrained(\n",
    "    repo_id='bartowski/aya-23-35B-GGUF',\n",
    "    filename='aya-23-35B-Q5_K_M.gguf',\n",
    "    n_gpu_layers=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    'gemma': gemma,\n",
    "    #'llama32': llama32,\n",
    "    #'llama31': llama31,\n",
    "    #'aya23': aya23,\n",
    "}"
   ],
   "id": "27d6d6528b3dcb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MetricX Setup",
   "id": "406e69e94455faa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[GitHub Repo to MetricX](https://github.com/google-research/metricx)",
   "id": "d75e732dc202507a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.137222Z",
     "start_time": "2025-01-20T09:50:45.133738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_metricx_score(source, reference, hypothesis):\n",
    "    '''\n",
    "    Calculates the MetricX-score based on source, reference, and hypothesis using metricx24.\n",
    "    We are currently using the metricx-24-hybrid-large-v2p6-bfloat16 model but there are also other options\n",
    "        as can be seen here: https://github.com/google-research/metricx\n",
    "\n",
    "    Args:\n",
    "        source: The source text (String).\n",
    "        reference: The reference translation (String).\n",
    "        hypothesis: The hypothesis translation (String).\n",
    "\n",
    "    Returns:\n",
    "        The calculated score as a float or None in case of an error.\n",
    "    '''\n",
    "\n",
    "    data = [{'id': '1', 'source': source, 'reference': reference, 'hypothesis': hypothesis}]\n",
    "\n",
    "    # Create temporary JSONL files\n",
    "    input_file = './temp_input.jsonl'\n",
    "    output_file = './temp_output.jsonl'\n",
    "    model = 'google/metricx-24-hybrid-large-v2p6-bfloat16'\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'w', encoding='utf-8') as f:\n",
    "            for entry in data:\n",
    "                json.dump(entry, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        command = [\n",
    "            'python', '-m', 'metricx24.predict',\n",
    "            '--tokenizer', 'google/mt5-xl',\n",
    "            '--model_name_or_path', model,\n",
    "            '--max_input_length', '1536',\n",
    "            '--batch_size', '1',\n",
    "            '--input_file', input_file,\n",
    "            '--output_file', output_file\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Capture output and errors (optional, can be useful for debugging)\n",
    "        # for line in process.stdout:\n",
    "        #     print(line, end='')\n",
    "        # for line in process.stderr:\n",
    "        #     print(f'ERROR: {line}', end='')\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f'Error executing metricx24. Return code: {process.returncode}')\n",
    "            return None\n",
    "\n",
    "        # Read score from the output file\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    output_data = json.loads(line)\n",
    "                    score = float(output_data.get('prediction'))  # Ensure that 'score' exists\n",
    "                    return score\n",
    "                except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "                    print('Error parsing the output file.')\n",
    "                    return None\n",
    "\n",
    "        return None  # If no valid line was found in the output file\n",
    "\n",
    "    finally:\n",
    "        # Remove temporary files\n",
    "        try:\n",
    "            os.remove(input_file)\n",
    "            os.remove(output_file)\n",
    "        except FileNotFoundError:\n",
    "            pass  #If the files don't exist for some reason, the error is caught\n"
   ],
   "id": "4a5ab47796005682",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.159234Z",
     "start_time": "2025-01-20T09:50:45.156814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example call\n",
    "source_text = 'I am learning Python for Machine Learning.'\n",
    "reference_text = 'I am learning Python for machine learning.'\n",
    "hypothesis_text = \"I'm studying Python for machine learning.\"\n",
    "\n",
    "# score = calculate_metricx_score(source_text, reference_text, hypothesis_text)\n",
    "#\n",
    "# if score is not None:\n",
    "#     print(f'The calculated score is: {score}')\n",
    "# else:\n",
    "#     print('The score calculation failed.')"
   ],
   "id": "f4c6295c686cc946",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.227550Z",
     "start_time": "2025-01-20T09:50:45.225684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## zweite Möglichkeit metricx zu nutzen...\n",
    "# import sys\n",
    "# from metricx24 import predict\n",
    "#\n",
    "# # Simuliere Kommandozeilenargumente\n",
    "# sys.argv = [\n",
    "#     'predict.py',  # Name des Skripts (wird ignoriert)\n",
    "#     '--tokenizer', 'google/mt5-xl',\n",
    "#     '--model_name_or_path', 'google/metricx-24-hybrid-large-v2p6-bfloat16',\n",
    "#     '--max_input_length', '1536',\n",
    "#     '--batch_size', '1',\n",
    "#     '--input_file', './input.jsonl',\n",
    "#     '--output_file', './output.jsonl'\n",
    "# ]\n",
    "#\n",
    "# # Rufe die main-Funktion auf\n",
    "# predict.main()\n",
    "#\n",
    "# print('Vorhersage abgeschlossen. Ergebnisse sind in 'output.jsonl' gespeichert.')"
   ],
   "id": "f492ee811ec3fd36",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "62ac1a6e2dbdae96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pipeline",
   "id": "8e5396fec902551e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Composition",
   "id": "4dba8b573c0379b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.233233Z",
     "start_time": "2025-01-20T09:50:45.231221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Beide Richtungen abbilden: English <-> German\n",
    "# TODO: Verschiedene Prompt Arten: zero-shot, few-shot und verschiedene Variationen reinbringen\n",
    "\n",
    "PROMPT_TEMPLATES_ENGLISH_GERMAN = {\n",
    "    'zero_shot-english': 'Please translate the following text from English to German: {text}',\n",
    "    #\"zero_shot-german\": f\"Bitte übersetze den folgenden Text von Englisch nach Deutsch: {text}\",\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from English to German.\n",
    "    #Here are some examples:\n",
    "    #- English: \"Hello\" -> German: \"Hallo\"\n",
    "    #- English: \"Goodbye\" -> German: \"Auf Wiedersehen\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte übersetze einen Text von Englisch nach Deutsch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Englisch: \"Hello\" -> Deutsch: \"Hallo\"\n",
    "    #- Englisch: \"Goodbye\" -> Deutsch: \"Auf Wiedersehen\"\n",
    "    #Jetzt übersetze diesen Text: {text}\"\"\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES_GERMAN_ENGLISH = {\n",
    "    \"zero_shot-english\": \"Please translate the following text from German to English: {text}\",\n",
    "    #\"zero_shot-german\": f\"Bitte übersetze den folgenden Text von Deutsch nach Englisch: {text}\",\n",
    "    #\"few-shot-english-1\": f\"\"\"Please translate a text from German to English.\n",
    "    #Here are some examples:\n",
    "    #- German: \"Hallo\" -> English: \"Hello\"\n",
    "    #- German: \"Auf Wiedersehen\" -> English: \"Goodbye\"\n",
    "    #Now translate this text: {text}\"\"\",\n",
    "    #\"few-shot-german-1\": f\"\"\"Bitte übersetze einen Text von Deutsch nach Englisch.\n",
    "    #Hier sind einige Beispiele:\n",
    "    #- Deutsch: \"Hallo\" -> Englisch: \"Hello\"\n",
    "    #- Deutsch: \"Auf Wiedersehen\" -> Englisch: \"Goodbye\"\n",
    "    #Jetzt übersetze diesen Text: {text}\"\"\",\n",
    "}"
   ],
   "id": "e70812e535e26c4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Interaction",
   "id": "cb00365dd1aad494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.289946Z",
     "start_time": "2025-01-20T09:50:45.288275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(model, prompt):\n",
    "    response = model(prompt, max_tokens=512, echo=False, stop=['Q:'])\n",
    "    return response['choices'][0]['text']"
   ],
   "id": "50fdfbda377216c8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "23525c106ed337fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.294526Z",
     "start_time": "2025-01-20T09:50:45.292823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_translation(source, reference, hypothesis):\n",
    "    #TODO: ich weiß nicht, ob wir die Methoden von sacrebleu richtig benutzen\n",
    "    #   oder ob wir die Strings vielleicht noch in einzelne Sätze splitten müssen\n",
    "    bleu_score = corpus_bleu([hypothesis], [[reference]]).score\n",
    "    chrf_score = sacrebleu.corpus_chrf([hypothesis], [[reference]]).score\n",
    "    metricx_score = calculate_metricx_score(source, reference, hypothesis)\n",
    "    return {'BLEU': bleu_score,\n",
    "            'chrF': chrf_score,\n",
    "            'MetricX': metricx_score}"
   ],
   "id": "68b3d386c2d373aa",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:51:54.327476Z",
     "start_time": "2025-01-20T09:51:54.320379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, duration_translation_generation,\n",
    "                  duration_metric_calculation, tmp_result):\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    if experiment:\n",
    "        if experiment.lifecycle_stage == 'deleted':\n",
    "            mlflow.tracking.MlflowClient().restore_experiment(experiment.experiment_id)\n",
    "            #mlflow.delete_experiment(experiment.experiment_id)\n",
    "    else:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f'{model_name}/{complexity}/{prompt_type}'):\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_param('complexity', complexity)\n",
    "        mlflow.log_param('prompt_type', prompt_type)\n",
    "        mlflow.log_param('duration_translation_generation', duration_translation_generation)\n",
    "        mlflow.log_param('duration_metric_calculation', duration_metric_calculation)\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        tmp_result.to_json('tmp_results.json', index=False)\n",
    "        mlflow.log_artifact('tmp_results.json')\n",
    "        mlflow.end_run()\n"
   ],
   "id": "770d3703528c2548",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Pipeline Method",
   "id": "36fde1001499c4c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:50:45.309084Z",
     "start_time": "2025-01-20T09:50:45.305879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline(texts):\n",
    "    results = pd.DataFrame(\n",
    "        columns=['model', 'complexity', 'prompt_type', 'prompt', 'source_text', 'hypothesis', 'reference', 'metrics'])\n",
    "    mlflow.set_tracking_uri(uri='http://127.0.0.1:5000')\n",
    "\n",
    "    for model_name, model in MODELS.items():\n",
    "        for _, row in texts.iterrows():\n",
    "\n",
    "            # Übersetzung Deutsch -> Englisch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_GERMAN_ENGLISH.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_german']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, results,\n",
    "                                                 row['text_german'], row['text_english'], template)\n",
    "\n",
    "            # Übersetzung Englisch -> Deutsch\n",
    "            for prompt_type, template in PROMPT_TEMPLATES_ENGLISH_GERMAN.items():\n",
    "                complexity = row['complexity']\n",
    "                if pd.notna(row['text_english']):\n",
    "                    results = execute_mlflow_run(complexity, model, model_name, prompt_type, results,\n",
    "                                                 row['text_english'], row['text_german'], template)\n",
    "\n",
    "    # results.to_csv('results.csv', index=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "def execute_mlflow_run(complexity, model, model_name, prompt_type, results, source_text, reference_text, template):\n",
    "    prompt = template.format(text=source_text)\n",
    "\n",
    "    start_time_translation = time.time()\n",
    "    hypothesis = translate(model, prompt)\n",
    "    end_time_translation = time.time()\n",
    "    metrics = evaluate_translation(source=source_text, reference=reference_text, hypothesis=hypothesis)\n",
    "    end_time_metrics = time.time()\n",
    "\n",
    "    tmp_result = pd.DataFrame([{\n",
    "                'model': model_name,\n",
    "                'complexity': complexity,\n",
    "                'prompt_type': prompt_type,\n",
    "                'prompt': prompt,\n",
    "                'source_text': source_text,\n",
    "                'hypothesis': hypothesis,\n",
    "                'reference_text': reference_text,\n",
    "                'metrics': metrics\n",
    "            }])\n",
    "\n",
    "    # MLflow-Logging\n",
    "    duration_translation_generation = round(end_time_translation - start_time_translation, 2)\n",
    "    duration_metric_calculation = round(end_time_metrics - end_time_translation, 2)\n",
    "    experiment_name = f'{model_name}_{complexity}'\n",
    "\n",
    "    log_to_mlflow(experiment_name, metrics, prompt_type, model_name, complexity, duration_translation_generation,\n",
    "                  duration_metric_calculation, tmp_result)\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    results = pd.concat([\n",
    "        results,\n",
    "        tmp_result\n",
    "    ], ignore_index=True)\n",
    "    return results"
   ],
   "id": "12fb076f8d17ce6a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Execute Pipeline",
   "id": "51c523705a1b59e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:55:37.218925Z",
     "start_time": "2025-01-20T09:51:58.556573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translation_results = run_pipeline(data)\n",
    "translation_results.to_csv('translation_results.csv')\n",
    "print('Pipeline abgeschlossen. Ergebnisse gespeichert.')"
   ],
   "id": "d34d572a8f458626",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2073) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gemma/easy/zero_shot-english at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/df6f32065ef5461f816b9da1279b1e8a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2096) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gemma/easy/zero_shot-english at: http://127.0.0.1:5000/#/experiments/429765178055128713/runs/d61e58d4191141568d93bcba893f105e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/429765178055128713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2125) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gemma/news_gen/zero_shot-english at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/8228b469f219483496e556555f4a549e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2140) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gemma/news_gen/zero_shot-english at: http://127.0.0.1:5000/#/experiments/391944840061747289/runs/f87754210575454994047f493de75082\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/391944840061747289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2179) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run gemma/news_spec/zero_shot-english at: http://127.0.0.1:5000/#/experiments/713590614913207437/runs/a5759109c8db475db064d886cd9e3c7b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/713590614913207437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m translation_results \u001B[38;5;241m=\u001B[39m run_pipeline(data)\n\u001B[1;32m      2\u001B[0m translation_results\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslation_results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPipeline abgeschlossen. Ergebnisse gespeichert.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[28], line 20\u001B[0m, in \u001B[0;36mrun_pipeline\u001B[0;34m(texts)\u001B[0m\n\u001B[1;32m     18\u001B[0m             complexity \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomplexity\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     19\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mnotna(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_english\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[0;32m---> 20\u001B[0m                 results \u001B[38;5;241m=\u001B[39m execute_mlflow_run(complexity, model, model_name, prompt_type, results,\n\u001B[1;32m     21\u001B[0m                                              row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_english\u001B[39m\u001B[38;5;124m'\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_german\u001B[39m\u001B[38;5;124m'\u001B[39m], template)\n\u001B[1;32m     23\u001B[0m results\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "Cell \u001B[0;32mIn[28], line 31\u001B[0m, in \u001B[0;36mexecute_mlflow_run\u001B[0;34m(complexity, model, model_name, prompt_type, results, source_text, reference_text, template)\u001B[0m\n\u001B[1;32m     28\u001B[0m prompt \u001B[38;5;241m=\u001B[39m template\u001B[38;5;241m.\u001B[39mformat(text\u001B[38;5;241m=\u001B[39msource_text)\n\u001B[1;32m     30\u001B[0m start_time_translation \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 31\u001B[0m hypothesis \u001B[38;5;241m=\u001B[39m translate(model, prompt)\n\u001B[1;32m     32\u001B[0m end_time_translation \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     33\u001B[0m metrics \u001B[38;5;241m=\u001B[39m evaluate_translation(source\u001B[38;5;241m=\u001B[39msource_text, reference\u001B[38;5;241m=\u001B[39mreference_text, hypothesis\u001B[38;5;241m=\u001B[39mhypothesis)\n",
      "Cell \u001B[0;32mIn[25], line 2\u001B[0m, in \u001B[0;36mtranslate\u001B[0;34m(model, prompt)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslate\u001B[39m(model, prompt):\n\u001B[0;32m----> 2\u001B[0m     response \u001B[38;5;241m=\u001B[39m model(prompt, max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, echo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, stop\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQ:\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:1899\u001B[0m, in \u001B[0;36mLlama.__call__\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1835\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m   1836\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1837\u001B[0m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     logit_bias: Optional[Dict[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1862\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001B[1;32m   1863\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate text from a prompt.\u001B[39;00m\n\u001B[1;32m   1864\u001B[0m \n\u001B[1;32m   1865\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1897\u001B[0m \u001B[38;5;124;03m        Response object containing the generated text.\u001B[39;00m\n\u001B[1;32m   1898\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_completion(\n\u001B[1;32m   1900\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m   1901\u001B[0m         suffix\u001B[38;5;241m=\u001B[39msuffix,\n\u001B[1;32m   1902\u001B[0m         max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m   1903\u001B[0m         temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m   1904\u001B[0m         top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m   1905\u001B[0m         min_p\u001B[38;5;241m=\u001B[39mmin_p,\n\u001B[1;32m   1906\u001B[0m         typical_p\u001B[38;5;241m=\u001B[39mtypical_p,\n\u001B[1;32m   1907\u001B[0m         logprobs\u001B[38;5;241m=\u001B[39mlogprobs,\n\u001B[1;32m   1908\u001B[0m         echo\u001B[38;5;241m=\u001B[39mecho,\n\u001B[1;32m   1909\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m   1910\u001B[0m         frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m   1911\u001B[0m         presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m   1912\u001B[0m         repeat_penalty\u001B[38;5;241m=\u001B[39mrepeat_penalty,\n\u001B[1;32m   1913\u001B[0m         top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m   1914\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1915\u001B[0m         seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m   1916\u001B[0m         tfs_z\u001B[38;5;241m=\u001B[39mtfs_z,\n\u001B[1;32m   1917\u001B[0m         mirostat_mode\u001B[38;5;241m=\u001B[39mmirostat_mode,\n\u001B[1;32m   1918\u001B[0m         mirostat_tau\u001B[38;5;241m=\u001B[39mmirostat_tau,\n\u001B[1;32m   1919\u001B[0m         mirostat_eta\u001B[38;5;241m=\u001B[39mmirostat_eta,\n\u001B[1;32m   1920\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m   1921\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[1;32m   1922\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[1;32m   1923\u001B[0m         grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[1;32m   1924\u001B[0m         logit_bias\u001B[38;5;241m=\u001B[39mlogit_bias,\n\u001B[1;32m   1925\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001B[0m, in \u001B[0;36mLlama.create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1830\u001B[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001B[38;5;241m=\u001B[39m completion_or_chunks\n\u001B[1;32m   1831\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m chunks\n\u001B[0;32m-> 1832\u001B[0m completion: Completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(completion_or_chunks)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   1833\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m completion\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001B[0m, in \u001B[0;36mLlama._create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1315\u001B[0m finish_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1316\u001B[0m multibyte_fix \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1317\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m   1318\u001B[0m     prompt_tokens,\n\u001B[1;32m   1319\u001B[0m     top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m   1320\u001B[0m     top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m   1321\u001B[0m     min_p\u001B[38;5;241m=\u001B[39mmin_p,\n\u001B[1;32m   1322\u001B[0m     typical_p\u001B[38;5;241m=\u001B[39mtypical_p,\n\u001B[1;32m   1323\u001B[0m     temp\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m   1324\u001B[0m     tfs_z\u001B[38;5;241m=\u001B[39mtfs_z,\n\u001B[1;32m   1325\u001B[0m     mirostat_mode\u001B[38;5;241m=\u001B[39mmirostat_mode,\n\u001B[1;32m   1326\u001B[0m     mirostat_tau\u001B[38;5;241m=\u001B[39mmirostat_tau,\n\u001B[1;32m   1327\u001B[0m     mirostat_eta\u001B[38;5;241m=\u001B[39mmirostat_eta,\n\u001B[1;32m   1328\u001B[0m     frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m   1329\u001B[0m     presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m   1330\u001B[0m     repeat_penalty\u001B[38;5;241m=\u001B[39mrepeat_penalty,\n\u001B[1;32m   1331\u001B[0m     stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[1;32m   1332\u001B[0m     logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[1;32m   1333\u001B[0m     grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[1;32m   1334\u001B[0m ):\n\u001B[1;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m llama_cpp\u001B[38;5;241m.\u001B[39mllama_token_is_eog(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mmodel, token):\n\u001B[1;32m   1336\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetokenize(completion_tokens, prev_tokens\u001B[38;5;241m=\u001B[39mprompt_tokens)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001B[0m, in \u001B[0;36mLlama.generate\u001B[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001B[0m\n\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# Eval and sample\u001B[39;00m\n\u001B[1;32m    908\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 909\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval(tokens)\n\u001B[1;32m    910\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m sample_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tokens:\n\u001B[1;32m    911\u001B[0m         token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[1;32m    912\u001B[0m             top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m    913\u001B[0m             top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    927\u001B[0m             idx\u001B[38;5;241m=\u001B[39msample_idx,\n\u001B[1;32m    928\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001B[0m, in \u001B[0;36mLlama.eval\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    639\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch\u001B[38;5;241m.\u001B[39mset_batch(\n\u001B[1;32m    641\u001B[0m     batch\u001B[38;5;241m=\u001B[39mbatch, n_past\u001B[38;5;241m=\u001B[39mn_past, logits_all\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_params\u001B[38;5;241m.\u001B[39mlogits_all\n\u001B[1;32m    642\u001B[0m )\n\u001B[0;32m--> 643\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ctx\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch)\n\u001B[1;32m    644\u001B[0m \u001B[38;5;66;03m# Save tokens\u001B[39;00m\n\u001B[1;32m    645\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_ids[n_past : n_past \u001B[38;5;241m+\u001B[39m n_tokens] \u001B[38;5;241m=\u001B[39m batch\n",
      "File \u001B[0;32m/opt/anaconda3/envs/softwarequality/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001B[0m, in \u001B[0;36mLlamaContext.decode\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: LlamaBatch):\n\u001B[0;32m--> 300\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m llama_cpp\u001B[38;5;241m.\u001B[39mllama_decode(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx,\n\u001B[1;32m    302\u001B[0m         batch\u001B[38;5;241m.\u001B[39mbatch,\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_decode returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "translation_results",
   "id": "4721ba5c1793230e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mlflow.end_run()",
   "id": "b911e753a5baa759",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
