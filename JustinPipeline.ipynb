{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompting Pipeline for comparison of different prompting techniques",
   "id": "7168eea69cea0132"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T13:26:18.218956Z",
     "start_time": "2024-12-26T13:26:18.206293Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import sacrebleu"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loading",
   "id": "c9652180add38161"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:30:06.883241Z",
     "start_time": "2024-12-26T13:30:06.875240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Daten laden\n",
    "text = pd.read_pickle('machine_translation.pkl')\n",
    "text\n",
    "\n",
    "# Standardisierung ?"
   ],
   "id": "8499598514c4a0c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    complexity                                        text_german  \\\n",
       "0         easy  Felix hat es satt: Ständig ist Mama unterwegs....   \n",
       "1     news_gen  Die rund 1.400 eingesetzten Beamten haben demn...   \n",
       "2    news_spec  Der Staatschef hat zugleich aber das Recht, vo...   \n",
       "3  pop_science  Dass der Klimawandel die Hitzewellen in Südasi...   \n",
       "4      science  Der DSA-110, der sich am Owens Valley Radio Ob...   \n",
       "\n",
       "                                        text_english  \n",
       "0  Felix is fed up: Mom is always on the go. But ...  \n",
       "1  The approximately 1,400 deployed officers have...  \n",
       "2  The head of state also has the right to appoin...  \n",
       "3  There is no question that climate change is in...  \n",
       "4  The DSA-110, situated at the Owens Valley Radi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity</th>\n",
       "      <th>text_german</th>\n",
       "      <th>text_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>Felix hat es satt: Ständig ist Mama unterwegs....</td>\n",
       "      <td>Felix is fed up: Mom is always on the go. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news_gen</td>\n",
       "      <td>Die rund 1.400 eingesetzten Beamten haben demn...</td>\n",
       "      <td>The approximately 1,400 deployed officers have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news_spec</td>\n",
       "      <td>Der Staatschef hat zugleich aber das Recht, vo...</td>\n",
       "      <td>The head of state also has the right to appoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pop_science</td>\n",
       "      <td>Dass der Klimawandel die Hitzewellen in Südasi...</td>\n",
       "      <td>There is no question that climate change is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Der DSA-110, der sich am Owens Valley Radio Ob...</td>\n",
       "      <td>The DSA-110, situated at the Owens Valley Radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Composition",
   "id": "6702e4d93a1af634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_prompt(text, mode=\"zero-shot\"):\n",
    "    if mode == \"zero-shot\":\n",
    "        return f\"Translate this text into German: {text}\"\n",
    "    elif mode == \"few-shot\":\n",
    "        examples = \"Example 1: Hello -> Hallo\\nExample 2: Goodbye -> Auf Wiedersehen\\n\"\n",
    "        return f\"{examples}Translate this text into German: {text}\""
   ],
   "id": "4f34cc94cb7c888f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Interaction",
   "id": "37deefa358457988"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modell laden\n",
    "translation_model = pipeline(\"translation_en_to_de\", model=\"lmstudio-ai/gemma-2b-it-GGUF\")\n",
    "\n",
    "# Prompt senden und Ergebnisse sammeln\n",
    "def translate_text(prompt):\n",
    "    result = translation_model(prompt)\n",
    "    return result[0][\"translation_text\"]"
   ],
   "id": "70ea73c439271270"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "ef5798fe874d276f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b07d83fbb232efc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:37:38.851724Z",
     "start_time": "2024-12-26T13:37:37.605241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Schritt 1: Daten laden\n",
    "def load_data(file_path):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    return data\n",
    "\n",
    "# Schritt 2: Prompt-Entwicklung\n",
    "def generate_prompts(data):\n",
    "    prompts = []\n",
    "    for _, row in data.iterrows():\n",
    "        text = row['text_english']\n",
    "        prompt = f\"Translate the following text to German:\\n\\n{text}\\n\\n\"\n",
    "        prompts.append(prompt)\n",
    "    data['prompt'] = prompts\n",
    "    return data\n",
    "\n",
    "# Schritt 3: Modell-Integration\n",
    "def load_models():\n",
    "    model_paths = {\n",
    "        \"gemma\": \"lmstudio-ai/gemma-2b-it-GGUF\",\n",
    "        \"llama_3_2\": \"hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF\",\n",
    "        \"llama_3_1\": \"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "        \"bartowski\": \"bartowski/aya-23-35B-GGUF\"\n",
    "    }\n",
    "    models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "        models[name] = {\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"model\": model\n",
    "        }\n",
    "    return models\n",
    "\n",
    "# Schritt 4: Modellinteraktion und MLflow-Logging\n",
    "def translate_with_models(data, models):\n",
    "    results = []\n",
    "\n",
    "    for model_name, components in models.items():\n",
    "        tokenizer = components['tokenizer']\n",
    "        model = components['model']\n",
    "        translation_pipeline = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"Translation with {model_name}\"):\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "            for index, prompt in enumerate(data['prompt']):\n",
    "                translation = translation_pipeline(prompt, max_length=512, truncation=True)\n",
    "                translated_text = translation[0]['translation_text']\n",
    "\n",
    "                # Log Prompt und Übersetzung\n",
    "                mlflow.log_metric(f\"translation_length_{index}\", len(translated_text))\n",
    "                mlflow.log_text(f\"Prompt {index}\", prompt)\n",
    "                mlflow.log_text(f\"Translation {index}\", translated_text)\n",
    "\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"translation\": translated_text\n",
    "                })\n",
    "\n",
    "            # Artefakte (Ergebnisse) speichern\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_path = f\"translation_results_{model_name}.csv\"\n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            mlflow.log_artifact(results_path)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Hauptpipeline\n",
    "def main_pipeline(file_path):\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")  # Lokales Tracking\n",
    "    experiment_name = \"Pipeline_FirstTest\"\n",
    "\n",
    "    if not mlflow.get_experiment_by_name(experiment_name):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Daten laden\n",
    "    data = load_data(file_path)\n",
    "    print(\"Daten geladen.\")\n",
    "\n",
    "    # Prompts erstellen\n",
    "    data = generate_prompts(data)\n",
    "    print(\"Prompts erstellt.\")\n",
    "\n",
    "    # Modelle laden\n",
    "    models = load_models()\n",
    "    print(\"Modelle geladen.\")\n",
    "\n",
    "    # Übersetzungen durchführen\n",
    "    translations = translate_with_models(data, models)\n",
    "    print(\"Übersetzungen durchgeführt.\")\n",
    "    return translations\n",
    "\n",
    "# Ausführung der Pipeline\n",
    "file_path = \"machine_translation.pkl\"\n",
    "results = main_pipeline(file_path)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results.to_csv(\"/mnt/data/translation_results.csv\", index=False)\n",
    "print(\"Ergebnisse gespeichert: translation_results.csv\")"
   ],
   "id": "a67611a4940149cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten geladen.\n",
      "Prompts erstellt.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'lmstudio-ai/gemma-2b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lmstudio-ai/gemma-2b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 103\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# Ausführung der Pipeline\u001B[39;00m\n\u001B[0;32m    102\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmachine_translation.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 103\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmain_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;66;03m# Ergebnisse speichern\u001B[39;00m\n\u001B[0;32m    106\u001B[0m results\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/data/translation_results.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[15], line 93\u001B[0m, in \u001B[0;36mmain_pipeline\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrompts erstellt.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# Modelle laden\u001B[39;00m\n\u001B[1;32m---> 93\u001B[0m models \u001B[38;5;241m=\u001B[39m \u001B[43mload_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModelle geladen.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Übersetzungen durchführen\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[15], line 31\u001B[0m, in \u001B[0;36mload_models\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m models \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, path \u001B[38;5;129;01min\u001B[39;00m model_paths\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 31\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     model \u001B[38;5;241m=\u001B[39m AutoModelForSeq2SeqLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(path)\n\u001B[0;32m     33\u001B[0m     models[name] \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m: tokenizer,\n\u001B[0;32m     35\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model\n\u001B[0;32m     36\u001B[0m     }\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\softwarequality\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:940\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    937\u001B[0m tokenizer_class_py, tokenizer_class_fast \u001B[38;5;241m=\u001B[39m TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[0;32m    939\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 940\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class_fast\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    941\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\softwarequality\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2013\u001B[0m \u001B[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001B[39;00m\n\u001B[0;32m   2014\u001B[0m \u001B[38;5;66;03m# loaded directly from the GGUF file.\u001B[39;00m\n\u001B[0;32m   2015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(full_file_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m full_file_name \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m gguf_file:\n\u001B[1;32m-> 2016\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m   2017\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load tokenizer for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load it from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2018\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the same name. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2019\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOtherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a directory \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2020\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontaining all relevant files for a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2021\u001B[0m     )\n\u001B[0;32m   2023\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_id, file_path \u001B[38;5;129;01min\u001B[39;00m vocab_files\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   2024\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m file_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files:\n",
      "\u001B[1;31mOSError\u001B[0m: Can't load tokenizer for 'lmstudio-ai/gemma-2b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lmstudio-ai/gemma-2b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e6b36650fee5638"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
